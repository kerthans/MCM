{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn import clone\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from typing import Dict, List, Tuple\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from rich.progress import Progress\n",
    "class OlympicMedalPredictor:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.models = {\n",
    "            'gbm': GradientBoostingRegressor(\n",
    "                n_estimators=200, \n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=6, \n",
    "                random_state=42\n",
    "            ),\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                min_child_samples=20,\n",
    "                min_child_weight=1e-3,\n",
    "                reg_lambda=1.0,\n",
    "                reg_alpha=0.0,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        self.model_weights = {}\n",
    "        self.feature_importance = {}\n",
    "        self.predictions_store = {}\n",
    "        \n",
    "    def prepare_data(self, features_df: pd.DataFrame, historical_data: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, pd.Series], List[str]]:\n",
    "        \"\"\"准备训练数据\"\"\"\n",
    "        df = features_df.merge(\n",
    "            historical_data[['NOC', 'Year', 'Gold', 'Total']], \n",
    "            on=['NOC', 'Year'], \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # 保存NOC列\n",
    "        noc_series = df['NOC']\n",
    "        \n",
    "        # 分离特征和目标变量\n",
    "        target_cols = ['Gold', 'Total']\n",
    "        exclude_cols = ['NOC'] + target_cols\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        X = df[feature_cols].astype(float)\n",
    "        y = {\n",
    "            'Gold': df['Gold'].astype(float),\n",
    "            'Total': df['Total'].astype(float)\n",
    "        }\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "    \n",
    "\n",
    "\n",
    "    def _optimize_ensemble_weights(self, \n",
    "                                predictions: Dict[str, np.ndarray], \n",
    "                                target: pd.Series,\n",
    "                                base_scores: Dict[str, float],\n",
    "                                constraints: Dict) -> Dict[str, float]:\n",
    "        \"\"\"优化集成权重，考虑历史约束\"\"\"\n",
    "        def objective(w):\n",
    "            w = w / w.sum()\n",
    "            ensemble_pred = np.zeros_like(list(predictions.values())[0])\n",
    "            for i, (_, pred) in enumerate(predictions.items()):\n",
    "                ensemble_pred += w[i] * pred\n",
    "            \n",
    "            # 添加约束惩罚\n",
    "            penalty = 0\n",
    "            if ensemble_pred.sum() < constraints['total_range'][0]:\n",
    "                penalty += (constraints['total_range'][0] - ensemble_pred.sum()) ** 2\n",
    "            elif ensemble_pred.sum() > constraints['total_range'][1]:\n",
    "                penalty += (ensemble_pred.sum() - constraints['total_range'][1]) ** 2\n",
    "            \n",
    "            return -r2_score(target, ensemble_pred) + penalty * 0.1\n",
    "        \n",
    "        n_models = len(predictions)\n",
    "        weights = np.array([base_scores[model] for model in predictions.keys()])\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        constraints_opt = (\n",
    "            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "        )\n",
    "        bounds = [(0, 1) for _ in range(n_models)]\n",
    "        \n",
    "        result = minimize(\n",
    "            objective, \n",
    "            weights, \n",
    "            method='SLSQP',\n",
    "            constraints=constraints_opt,\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': 1000}\n",
    "        )\n",
    "        \n",
    "        optimized_weights = result.x / result.x.sum()\n",
    "        return dict(zip(predictions.keys(), optimized_weights))\n",
    "    \n",
    "    def _calculate_weights(self, scores: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"计算模型权重\"\"\"\n",
    "        total = sum(scores.values())\n",
    "        weights = {model: score/total for model, score in scores.items()}\n",
    "        return weights\n",
    "    \n",
    "    def train_models(self, X: pd.DataFrame, y: Dict[str, pd.Series]) -> Dict:\n",
    "        trained_models = {}\n",
    "        scores = {}\n",
    "        \n",
    "        historical_constraints = {\n",
    "            'Gold': {\n",
    "                'top_countries': {\n",
    "                    'China': (35, 45), 'United States': (30, 40),\n",
    "                    'Great Britain': (20, 30), 'ROC': (20, 30), \n",
    "                    'Japan': (20, 30)\n",
    "                },\n",
    "                'min': 0, 'max': 45, 'total_range': (300, 340)\n",
    "            },\n",
    "            'Total': {\n",
    "                'top_countries': {\n",
    "                    'China': (80, 100), 'United States': (80, 100),\n",
    "                    'Great Britain': (50, 70), 'ROC': (50, 70),\n",
    "                    'Japan': (50, 70)\n",
    "                },\n",
    "                'min': 0, 'max': 100, 'total_range': (950, 1100)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with Progress() as progress:\n",
    "            total_tasks = len(y) * len(self.models)\n",
    "            train_progress = progress.add_task(\"[cyan]训练模型...\", total=total_tasks)\n",
    "            \n",
    "            for target_name, target in y.items():\n",
    "                if target is None:\n",
    "                    continue\n",
    "                    \n",
    "                trained_models[target_name] = {}\n",
    "                scores[target_name] = {}\n",
    "                self.predictions_store[target_name] = {}\n",
    "                self.model_weights[target_name] = {}\n",
    "                \n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                \n",
    "                for model_name, model in self.models.items():\n",
    "                    model_scores = []\n",
    "                    fold_predictions = []\n",
    "                    fold_actuals = []\n",
    "                    \n",
    "                    # 模型参数设置\n",
    "                    if model_name == 'xgb':\n",
    "                        model.set_params(learning_rate=0.05, n_estimators=300)\n",
    "                    elif model_name == 'lgb':\n",
    "                        model.set_params(learning_rate=0.05, n_estimators=300) \n",
    "                    elif model_name == 'gbm':\n",
    "                        model.set_params(learning_rate=0.05, n_estimators=300)\n",
    "                    elif model_name == 'rf':\n",
    "                        model.set_params(n_estimators=300)\n",
    "                    \n",
    "                    for train_idx, val_idx in tscv.split(X):\n",
    "                        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                        y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n",
    "                        \n",
    "                        # 模型训练\n",
    "                        if model_name in ['xgb', 'lgb']:\n",
    "                            model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "                        else:\n",
    "                            model.fit(X_train, y_train)\n",
    "                        \n",
    "                        pred = model.predict(X_val)\n",
    "                        pred = np.clip(\n",
    "                            pred,\n",
    "                            historical_constraints[target_name]['min'],\n",
    "                            historical_constraints[target_name]['max']\n",
    "                        )\n",
    "                        \n",
    "                        fold_predictions.extend(pred)\n",
    "                        fold_actuals.extend(y_val)\n",
    "                        score = r2_score(y_val, pred)\n",
    "                        model_scores.append(score)\n",
    "                    \n",
    "                    trained_models[target_name][model_name] = model\n",
    "                    scores[target_name][model_name] = np.mean(model_scores)\n",
    "                    \n",
    "                    if hasattr(model, 'feature_importances_'):\n",
    "                        self.feature_importance[f\"{target_name}_{model_name}\"] = pd.Series(\n",
    "                            model.feature_importances_,\n",
    "                            index=X.columns\n",
    "                        ).sort_values(ascending=False)\n",
    "                    \n",
    "                    progress.update(train_progress, advance=1)\n",
    "                \n",
    "                weights = self._optimize_ensemble_weights(\n",
    "                    {name: model.predict(X) for name, model in trained_models[target_name].items()},\n",
    "                    target,\n",
    "                    scores[target_name],\n",
    "                    historical_constraints[target_name]\n",
    "                )\n",
    "                self.model_weights[target_name].update(weights)\n",
    "        \n",
    "        return trained_models\n",
    "\n",
    "    def _calculate_optimal_weights(self, predictions: Dict, scores: Dict) -> Dict[str, float]:\n",
    "        \"\"\"计算最优权重\"\"\"\n",
    "        base_weights = np.array([scores[model] for model in predictions.keys()])\n",
    "        weights = base_weights / np.sum(base_weights)\n",
    "        \n",
    "        # 验证预测结果的表现\n",
    "        ensemble_predictions = np.zeros_like(list(predictions.values())[0]['predictions'])\n",
    "        for i, (model_name, _) in enumerate(predictions.items()):\n",
    "            ensemble_predictions += weights[i] * predictions[model_name]['predictions']\n",
    "        \n",
    "        # 返回归一化的权重\n",
    "        return dict(zip(predictions.keys(), weights))\n",
    "\n",
    "    def predict_with_uncertainty(self, \n",
    "                            X_pred: pd.DataFrame, \n",
    "                            models: Dict, \n",
    "                            target_name: str,\n",
    "                            n_iterations: int = 500) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"改进的不确定性预测函数，添加历史约束\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # 历史数据约束\n",
    "        historical_constraints = {\n",
    "            'Gold': {\n",
    "                'top_countries': {\n",
    "                    'China': (35, 45),\n",
    "                    'United States': (30, 40),\n",
    "                    'Great Britain': (20, 30),\n",
    "                    'ROC': (20, 30),\n",
    "                    'Japan': (20, 30)\n",
    "                },\n",
    "                'min': 0,\n",
    "                'max': 45,\n",
    "                'std_scale': 0.1\n",
    "            },\n",
    "            'Total': {\n",
    "                'top_countries': {\n",
    "                    'China': (80, 100),\n",
    "                    'United States': (80, 100),\n",
    "                    'Great Britain': (50, 70),\n",
    "                    'ROC': (50, 70),\n",
    "                    'Japan': (50, 70)\n",
    "                },\n",
    "                'min': 0,\n",
    "                'max': 100,\n",
    "                'std_scale': 0.15\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with Progress() as progress:\n",
    "            predict_task = progress.add_task(\"[cyan]生成预测...\", total=n_iterations)\n",
    "            \n",
    "            for _ in range(n_iterations):\n",
    "                model_preds = {}\n",
    "                for model_name, model in models[target_name].items():\n",
    "                    # Bootstrap sampling with noise\n",
    "                    indices = np.random.choice(len(X_pred), size=len(X_pred), replace=True)\n",
    "                    X_sample = X_pred.iloc[indices]\n",
    "                    \n",
    "                    # 特征噪声\n",
    "                    noise_scale = 0.03\n",
    "                    feature_noise = np.random.normal(0, noise_scale, size=X_sample.shape)\n",
    "                    X_noisy = X_sample + feature_noise * X_sample.std().values\n",
    "                    \n",
    "                    # 预测\n",
    "                    pred = model.predict(X_noisy)\n",
    "                    \n",
    "                    # 应用历史约束\n",
    "                    for country_idx, country in enumerate(X_pred.index):\n",
    "                        if country in historical_constraints[target_name]['top_countries']:\n",
    "                            min_val, max_val = historical_constraints[target_name]['top_countries'][country]\n",
    "                            pred[country_idx] = np.clip(pred[country_idx], min_val, max_val)\n",
    "                        else:\n",
    "                            pred[country_idx] = np.clip(\n",
    "                                pred[country_idx],\n",
    "                                historical_constraints[target_name]['min'],\n",
    "                                historical_constraints[target_name]['max']\n",
    "                            )\n",
    "                    \n",
    "                    model_preds[model_name] = pred\n",
    "                \n",
    "                # 集成预测\n",
    "                ensemble_pred = sum(\n",
    "                    pred * self.model_weights[target_name][model_name]\n",
    "                    for model_name, pred in model_preds.items()\n",
    "                )\n",
    "                \n",
    "                predictions.append(ensemble_pred)\n",
    "                progress.update(predict_task, advance=1)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        mean_pred = predictions.mean(axis=0)\n",
    "        std_pred = predictions.std(axis=0)\n",
    "        \n",
    "        # 调整不确定性估计\n",
    "        for i, country in enumerate(X_pred.index):\n",
    "            if country in historical_constraints[target_name]['top_countries']:\n",
    "                std_pred[i] *= historical_constraints[target_name]['std_scale']\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "    def identify_first_time_medals(self, \n",
    "                                 predictions: np.ndarray, \n",
    "                                 historical_data: pd.DataFrame,\n",
    "                                 countries: List[str]) -> List[str]:\n",
    "        \"\"\"识别可能首次获得奖牌的国家\"\"\"\n",
    "        # 获取历史上从未获得奖牌的国家\n",
    "        historical_medals = historical_data.groupby('NOC')['Total'].sum()\n",
    "        never_medaled = set(countries) - set(historical_medals[historical_medals > 0].index)\n",
    "        \n",
    "        # 预测值大于阈值的国家可能首次获得奖牌\n",
    "        threshold = 0.5  # 可调整的阈值\n",
    "        first_time_medals = []\n",
    "        \n",
    "        for country, pred in zip(countries, predictions):\n",
    "            if country in never_medaled and pred > threshold:\n",
    "                first_time_medals.append(country)\n",
    "        \n",
    "        return first_time_medals\n",
    "    \n",
    "    def predict_2028_olympics(self, features_df: pd.DataFrame, historical_data: pd.DataFrame) -> None:\n",
    "        \"\"\"预测2028年奥运会奖牌情况\"\"\"\n",
    "        try:\n",
    "            # 准备数据\n",
    "            X, y, feature_cols = self.prepare_data(features_df, historical_data)\n",
    "            \n",
    "            # 训练模型\n",
    "            self.console.print(\"\\n[bold cyan]训练模型中...[/bold cyan]\")\n",
    "            trained_models = self.train_models(X, y)\n",
    "            \n",
    "            # 准备2028年预测数据\n",
    "            X_2028 = self._prepare_2028_features(features_df[['NOC'] + feature_cols])\n",
    "            \n",
    "            # 获取预测用的特征矩阵(不含NOC)\n",
    "            X_2028_features = X_2028[feature_cols]\n",
    "            \n",
    "            # 预测并计算不确定性\n",
    "            self.console.print(\"\\n[bold cyan]生成2028年预测...[/bold cyan]\")\n",
    "            results = {}\n",
    "            for target in ['Gold', 'Total']:\n",
    "                if target in trained_models:\n",
    "                    mean_pred, std_pred = self.predict_with_uncertainty(\n",
    "                        X_2028_features, \n",
    "                        trained_models, \n",
    "                        target\n",
    "                    )\n",
    "                    results[target] = {\n",
    "                        'predictions': mean_pred,\n",
    "                        'uncertainty': std_pred\n",
    "                    }\n",
    "            \n",
    "            # 识别可能首次获得奖牌的国家\n",
    "            first_time_medalists = self.identify_first_time_medals(\n",
    "                results['Total']['predictions'],\n",
    "                historical_data,\n",
    "                X_2028['NOC'].unique()\n",
    "            )\n",
    "            \n",
    "            # 输出预测结果\n",
    "            self._display_predictions(results, X_2028['NOC'].unique(), first_time_medalists)\n",
    "            \n",
    "            # 保存模型和预测结果\n",
    "            self._save_results(trained_models, results, X_2028['NOC'].unique())\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]预测过程中出现错误: {str(e)}[/bold red]\")\n",
    "            raise e\n",
    "    \n",
    "    def _prepare_2028_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"准备2028年的特征数据\"\"\"\n",
    "        # 复制最近一年的数据作为基础\n",
    "        latest_year = X['Year'].max()\n",
    "        X_2028 = X[X['Year'] == latest_year].copy()\n",
    "        \n",
    "        # 更新年份\n",
    "        X_2028['Year'] = 2028\n",
    "        \n",
    "        # 更新时间相关特征\n",
    "        time_features = ['Years_To_Next', 'Olympics_Since_Last']\n",
    "        for col in time_features:\n",
    "            if col in X_2028.columns:\n",
    "                X_2028[col] = 4  # 假设是正常的奥运周期\n",
    "        \n",
    "        # 更新参与次数\n",
    "        if 'Participation_Count' in X_2028.columns:\n",
    "            X_2028['Participation_Count'] += 1\n",
    "        \n",
    "        return X_2028\n",
    "    \n",
    "    def _display_predictions(self, \n",
    "                        results: Dict, \n",
    "                        countries: np.ndarray, \n",
    "                        first_time_medalists: List[str]) -> None:\n",
    "        \"\"\"显示预测结果\"\"\"\n",
    "        # 创建结果表格\n",
    "        table = Table(title=\"2028洛杉矶奥运会奖牌预测\")\n",
    "        table.add_column(\"国家\")\n",
    "        table.add_column(\"预计金牌数\")\n",
    "        table.add_column(\"预计总奖牌数\")\n",
    "        table.add_column(\"预测不确定性\")\n",
    "        \n",
    "        # 将countries转换为列表以使用index方法\n",
    "        countries_list = list(countries)\n",
    "        \n",
    "        for i, country in enumerate(countries_list):\n",
    "            gold_pred = f\"{results['Gold']['predictions'][i]:.1f}\"\n",
    "            gold_std = f\"±{results['Gold']['uncertainty'][i]:.1f}\"\n",
    "            total_pred = f\"{results['Total']['predictions'][i]:.1f}\"\n",
    "            total_std = f\"±{results['Total']['uncertainty'][i]:.1f}\"\n",
    "            \n",
    "            table.add_row(\n",
    "                country,\n",
    "                f\"{gold_pred} ({gold_std})\",\n",
    "                f\"{total_pred} ({total_std})\",\n",
    "                \"高\" if country in first_time_medalists else \"中\"\n",
    "            )\n",
    "        \n",
    "        self.console.print(table)\n",
    "        \n",
    "        # 显示首次获奖国家\n",
    "        if first_time_medalists:\n",
    "            self.console.print(\"\\n[bold green]预计首次获得奖牌的国家:[/bold green]\")\n",
    "            for country in first_time_medalists:\n",
    "                self.console.print(f\"- {country}\")\n",
    "    \n",
    "    def _save_results(self, \n",
    "                    models: Dict, \n",
    "                    results: Dict, \n",
    "                    countries: List[str]) -> None:\n",
    "        \"\"\"保存模型和预测结果\"\"\"\n",
    "        # 创建保存目录\n",
    "        save_dir = Path(\"models\")\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 保存模型\n",
    "        for target, target_models in models.items():\n",
    "            for model_name, model in target_models.items():\n",
    "                joblib.dump(\n",
    "                    model, \n",
    "                    save_dir / f\"{target}_{model_name}_model.joblib\"\n",
    "                )\n",
    "        \n",
    "        # 保存预测结果\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'Country': countries,\n",
    "            'Predicted_Gold': results['Gold']['predictions'],\n",
    "            'Gold_Uncertainty': results['Gold']['uncertainty'],\n",
    "            'Predicted_Total': results['Total']['predictions'],\n",
    "            'Total_Uncertainty': results['Total']['uncertainty']\n",
    "        })\n",
    "        \n",
    "        # 同时保存为 CSV 和 Parquet 格式\n",
    "        predictions_df.to_csv(save_dir / \"predictions_2028.csv\", index=False)\n",
    "        predictions_df.to_parquet(save_dir / \"predictions_2028.parquet\", index=False)\n",
    "        \n",
    "        # 保存特征重要性\n",
    "        importance_df = pd.DataFrame(self.feature_importance)\n",
    "        importance_df.to_csv(save_dir / \"feature_importance.csv\", index=True)\n",
    "        importance_df.to_parquet(save_dir / \"feature_importance.parquet\", index=True)\n",
    "\n",
    "    def generate_summary_report(self, predictions_df: pd.DataFrame, historical_data: pd.DataFrame) -> str:\n",
    "        \"\"\"生成详细的预测评估报告\"\"\"\n",
    "        summary = []\n",
    "        \n",
    "        # 1. 基本统计分析\n",
    "        total_countries = len(predictions_df)\n",
    "        avg_gold = predictions_df['Predicted_Gold'].mean()\n",
    "        avg_total = predictions_df['Predicted_Total'].mean()\n",
    "        total_gold = predictions_df['Predicted_Gold'].sum()\n",
    "        total_medals = predictions_df['Predicted_Total'].sum()\n",
    "        \n",
    "        summary.append(\"1. 基本统计分析\")\n",
    "        summary.append(f\"   - 预测国家数量: {total_countries}\")\n",
    "        summary.append(f\"   - 平均预测金牌数: {avg_gold:.2f}\")\n",
    "        summary.append(f\"   - 平均预测总奖牌数: {avg_total:.2f}\")\n",
    "        summary.append(f\"   - 预测总金牌数: {total_gold:.2f}\")\n",
    "        summary.append(f\"   - 预测总奖牌数: {total_medals:.2f}\")\n",
    "        \n",
    "        # 2. 历史趋势分析\n",
    "        recent_years = historical_data['Year'].unique()[-3:]\n",
    "        historical_trends = []\n",
    "        for year in recent_years:\n",
    "            year_data = historical_data[historical_data['Year'] == year]\n",
    "            historical_trends.append({\n",
    "                'year': year,\n",
    "                'total_gold': year_data['Gold'].sum(),\n",
    "                'total_medals': year_data['Total'].sum()\n",
    "            })\n",
    "        \n",
    "        summary.append(\"\\n2. 历史趋势分析\")\n",
    "        for trend in historical_trends:\n",
    "            summary.append(f\"   - {trend['year']}年:\")\n",
    "            summary.append(f\"     * 总金牌数: {trend['total_gold']}\")\n",
    "            summary.append(f\"     * 总奖牌数: {trend['total_medals']}\")\n",
    "        \n",
    "        # 3. 预测可信度评估\n",
    "        gold_uncertainty = predictions_df['Gold_Uncertainty'].mean()\n",
    "        total_uncertainty = predictions_df['Total_Uncertainty'].mean()\n",
    "        \n",
    "        summary.append(\"\\n3. 预测可信度评估\")\n",
    "        summary.append(f\"   - 金牌预测平均不确定性: ±{gold_uncertainty:.2f}\")\n",
    "        summary.append(f\"   - 总奖牌预测平均不确定性: ±{total_uncertainty:.2f}\")\n",
    "        \n",
    "        # 4. 主要发现\n",
    "        summary.append(\"\\n4. 主要发现\")\n",
    "        summary.append(\"   - 预测趋势与历史数据对比\")\n",
    "        summary.append(\"   - 国家间竞争格局变化\")\n",
    "        summary.append(\"   - 新兴运动强国分析\")\n",
    "        \n",
    "        # 5. 预测局限性\n",
    "        summary.append(\"\\n5. 预测局限性\")\n",
    "        summary.append(\"   - 模型假设和约束\")\n",
    "        summary.append(\"   - 不确定性来源\")\n",
    "        summary.append(\"   - 潜在影响因素\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "def main():\n",
    "    console = Console()\n",
    "    \n",
    "    try:\n",
    "        # 创建保存目录\n",
    "        Path(\"models\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # 加载数据\n",
    "        console.print(\"[bold cyan]加载数据...[/bold cyan]\")\n",
    "        \n",
    "        # 尝试不同的数据加载方式\n",
    "        def load_data(file_path_base):\n",
    "            \"\"\"尝试多种方式加载数据\"\"\"\n",
    "            # 尝试不同的文件扩展名和编码\n",
    "            attempts = [\n",
    "                (f\"{file_path_base}.parquet\", lambda x: pd.read_parquet(x)),\n",
    "                (f\"{file_path_base}.csv\", lambda x: pd.read_csv(x)),\n",
    "                (f\"{file_path_base}.csv\", lambda x: pd.read_csv(x, encoding='utf-8')),\n",
    "                (f\"{file_path_base}.csv\", lambda x: pd.read_csv(x, encoding='latin1'))\n",
    "            ]\n",
    "            \n",
    "            last_error = None\n",
    "            for file_path, reader in attempts:\n",
    "                try:\n",
    "                    if Path(file_path).exists():\n",
    "                        data = reader(file_path)\n",
    "                        console.print(f\"[green]成功从 {file_path} 加载数据[/green]\")\n",
    "                        return data\n",
    "                except Exception as e:\n",
    "                    last_error = e\n",
    "                    continue\n",
    "            \n",
    "            raise FileNotFoundError(f\"无法加载数据文件 {file_path_base}.*\\n最后的错误: {str(last_error)}\")\n",
    "        \n",
    "        # 加载特征数据\n",
    "        features_df = load_data(\"data/processed/features\")\n",
    "        historical_data = load_data(\"data/processed/medal_counts\")\n",
    "        \n",
    "        # 数据验证\n",
    "        required_columns = ['Year', 'NOC', 'Gold', 'Total']\n",
    "        for col in required_columns:\n",
    "            if col not in historical_data.columns:\n",
    "                raise ValueError(f\"历史数据缺少必要的列: {col}\")\n",
    "        \n",
    "        # 显示数据基本信息\n",
    "        console.print(\"\\n[bold green]数据加载完成[/bold green]\")\n",
    "        console.print(f\"特征数据形状: {features_df.shape}\")\n",
    "        console.print(f\"特征列: {', '.join(features_df.columns)}\")\n",
    "        console.print(f\"历史数据形状: {historical_data.shape}\")\n",
    "        console.print(f\"历史数据列: {', '.join(historical_data.columns)}\")\n",
    "        \n",
    "        # 检查数据质量\n",
    "        console.print(\"\\n[bold cyan]检查数据质量...[/bold cyan]\")\n",
    "        \n",
    "        # 检查缺失值\n",
    "        missing_features = features_df.isnull().sum()\n",
    "        if missing_features.any():\n",
    "            console.print(\"[yellow]特征数据中存在缺失值:[/yellow]\")\n",
    "            console.print(missing_features[missing_features > 0])\n",
    "        \n",
    "        missing_historical = historical_data.isnull().sum()\n",
    "        if missing_historical.any():\n",
    "            console.print(\"[yellow]历史数据中存在缺失值:[/yellow]\")\n",
    "            console.print(missing_historical[missing_historical > 0])\n",
    "        \n",
    "        # 初始化预测器\n",
    "        predictor = OlympicMedalPredictor()\n",
    "        \n",
    "        # 运行预测\n",
    "        predictor.predict_2028_olympics(features_df, historical_data)\n",
    "        # 加载预测结果\n",
    "        predictions_df = pd.read_parquet(\"models/predictions_2028.parquet\")\n",
    "        \n",
    "        # 生成摘要报告\n",
    "        summary_report = predictor.generate_summary_report(predictions_df, historical_data)\n",
    "        \n",
    "        # 保存报告\n",
    "        with open(\"models/prediction_summary_report.txt\", \"w\") as f:\n",
    "            f.write(summary_report)\n",
    "        \n",
    "        console.print(\"\\n[bold cyan]预测评估摘要:[/bold cyan]\")\n",
    "        console.print(summary_report)\n",
    "        # 保存结果\n",
    "        console.print(\"\\n[bold green]预测完成！结果已保存到 models 目录[/bold green]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]错误: {str(e)}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class CountryInsight:\n",
    "    noc: str\n",
    "    trend_score: float\n",
    "    stability_score: float\n",
    "    diversity_score: float\n",
    "    key_findings: List[str]\n",
    "    recommendations: List[str]\n",
    "\n",
    "class OlympicMedalInsightAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.insights = {}\n",
    "        self.trends = {}\n",
    "        self.patterns = {}\n",
    "        \n",
    "    def load_and_prepare_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"加载和准备分析所需的数据\"\"\"\n",
    "        try:\n",
    "            def try_load_data(file_path_base: str) -> pd.DataFrame:\n",
    "                \"\"\"尝试多种方式加载数据\"\"\"\n",
    "                base_path = Path(file_path_base)\n",
    "                data_dir = base_path.parent\n",
    "                data_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                attempts = [\n",
    "                    (base_path.with_suffix('.parquet'), lambda x: pd.read_parquet(x)),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x)),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x, encoding='utf-8')),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x, encoding='latin1')),\n",
    "                    (base_path.parent / f\"{base_path.name}.parquet\", lambda x: pd.read_parquet(x)),\n",
    "                    (base_path.parent / f\"{base_path.name}.csv\", lambda x: pd.read_csv(x)),\n",
    "                    (base_path.parent / f\"{base_path.name}.xlsx\", lambda x: pd.read_excel(x))\n",
    "                ]\n",
    "                \n",
    "                errors = []\n",
    "                for file_path, reader in attempts:\n",
    "                    try:\n",
    "                        if file_path.exists():\n",
    "                            data = reader(file_path)\n",
    "                            self.console.print(f\"[green]成功从 {file_path} 加载数据[/green]\")\n",
    "                            return data\n",
    "                    except Exception as e:\n",
    "                        errors.append(f\"{file_path}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                available_files = list(data_dir.glob(\"*\")) if data_dir.exists() else []\n",
    "                files_str = \"\\n\".join(f\"- {f.name}\" for f in available_files) if available_files else \"目录为空\"\n",
    "                \n",
    "                error_msg = (\n",
    "                    f\"无法加载数据文件 {base_path}.*\\n\"\n",
    "                    f\"尝试的路径:\\n{chr(10).join(f'- {err}' for err in errors)}\\n\"\n",
    "                    f\"目录 {data_dir} 中的文件:\\n{files_str}\"\n",
    "                )\n",
    "                raise FileNotFoundError(error_msg)\n",
    "\n",
    "            # 加载数据\n",
    "            medals_df = try_load_data(\"data/processed/medal_counts\")\n",
    "            athletes_df = try_load_data(\"data/processed/athletes\")\n",
    "            programs_df = try_load_data(\"data/processed/programs\")\n",
    "            \n",
    "            # 数据预处理\n",
    "            for df in [medals_df, athletes_df, programs_df]:\n",
    "                if 'Year' in df.columns:\n",
    "                    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "                    \n",
    "            # 移除重复项\n",
    "            medals_df = medals_df.drop_duplicates()\n",
    "            athletes_df = athletes_df.drop_duplicates()\n",
    "            programs_df = programs_df.drop_duplicates()\n",
    "            \n",
    "            # 验证必要的列\n",
    "            required_cols = {\n",
    "                'medals_df': ['Year', 'NOC', 'Gold', 'Total'],\n",
    "                'athletes_df': ['Year', 'NOC', 'Sport'],\n",
    "                'programs_df': ['Sport', 'Discipline']\n",
    "            }\n",
    "            \n",
    "            for df_name, df in [\n",
    "                ('medals_df', medals_df), \n",
    "                ('athletes_df', athletes_df), \n",
    "                ('programs_df', programs_df)\n",
    "            ]:\n",
    "                missing_cols = [col for col in required_cols[df_name] if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f\"{df_name} 缺少必要的列: {', '.join(missing_cols)}\")\n",
    "            \n",
    "            return medals_df, athletes_df, programs_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]数据加载错误: {str(e)}[/bold red]\")\n",
    "            raise\n",
    "\n",
    "    def analyze_medal_trends(self, medals_df: pd.DataFrame, athletes_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"分析奖牌趋势和模式\"\"\"\n",
    "        # 预处理：过滤历史国家\n",
    "        historical_nocs = {\n",
    "            'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC',\n",
    "            'Soviet Union', 'East Germany', 'West Germany', 'Unified Team'\n",
    "        }\n",
    "        medals_df = medals_df[~medals_df['NOC'].isin(historical_nocs)].copy()\n",
    "        athletes_df = athletes_df[~athletes_df['NOC'].isin(historical_nocs)].copy()\n",
    "        trends = {}\n",
    "        \n",
    "        # 1. 整体趋势分析\n",
    "        overall_trends = {\n",
    "            'total_countries': medals_df['NOC'].nunique(),\n",
    "            'medals_concentration': self._calculate_medals_concentration(medals_df),\n",
    "            'emerging_countries': self._identify_emerging_countries(medals_df),\n",
    "            'declining_countries': self._identify_declining_countries(medals_df)\n",
    "        }\n",
    "        trends['overall'] = overall_trends\n",
    "        \n",
    "        # 2. 区域性分析\n",
    "        region_mapping = self._create_region_mapping()\n",
    "        medals_df['Region'] = medals_df['NOC'].map(region_mapping)\n",
    "        regional_trends = self._analyze_regional_patterns(medals_df)\n",
    "        trends['regional'] = regional_trends\n",
    "        \n",
    "        # 3. 项目多样性分析 - 使用 athletes_df 而不是 medals_df\n",
    "        diversity_trends = self._analyze_sport_diversity(athletes_df)\n",
    "        trends['diversity'] = diversity_trends\n",
    "        \n",
    "        return trends\n",
    "\n",
    "    def _calculate_medals_concentration(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"计算奖牌集中度趋势\"\"\"\n",
    "        concentration = {}\n",
    "        \n",
    "        # 按时期分析\n",
    "        periods = [(1896, 1950), (1951, 2000), (2001, 2024)]\n",
    "        \n",
    "        for start, end in periods:\n",
    "            period_data = data[(data['Year'] >= start) & (data['Year'] <= end)]\n",
    "            total_medals = period_data.groupby('NOC')['Total'].sum()\n",
    "            \n",
    "            # 计算基尼系数\n",
    "            gini = self._calculate_gini(total_medals.values)\n",
    "            \n",
    "            # 计算前10国家占比\n",
    "            top_10_share = total_medals.nlargest(10).sum() / total_medals.sum()\n",
    "            \n",
    "            concentration[f\"{start}-{end}\"] = {\n",
    "                'gini_coefficient': gini,\n",
    "                'top_10_share': top_10_share,\n",
    "                'total_countries': len(total_medals)\n",
    "            }\n",
    "            \n",
    "        return concentration\n",
    "\n",
    "    def _calculate_gini(self, array: np.ndarray) -> float:\n",
    "        \"\"\"计算基尼系数\"\"\"\n",
    "        array = array.flatten()\n",
    "        if len(array) == 0:\n",
    "            return 0\n",
    "        array = np.sort(array)\n",
    "        index = np.arange(1, len(array) + 1)\n",
    "        n = len(array)\n",
    "        return ((2 * index - n - 1) * array).sum() / (n * array.sum())\n",
    "    def _identify_emerging_countries(self, data: pd.DataFrame, recent_years: int = 12) -> List[Dict]:\n",
    "        \"\"\"改进的新兴国家识别算法，使用复合指标\"\"\"\n",
    "        historical_nocs = {'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC'}\n",
    "        data = data[~data['NOC'].isin(historical_nocs)].copy()\n",
    "        \n",
    "        def calculate_growth_score(early_medals: float, recent_medals: float) -> float:\n",
    "            \"\"\"使用对数增长率避免除零问题\"\"\"\n",
    "            if early_medals == 0:\n",
    "                early_medals = 0.5  # Laplace平滑\n",
    "            return np.log((recent_medals + 1) / (early_medals + 1))\n",
    "        \n",
    "        def calculate_momentum_score(country_data: pd.DataFrame) -> float:\n",
    "            \"\"\"计算动量得分\"\"\"\n",
    "            if len(country_data) < 3:\n",
    "                return 0\n",
    "            \n",
    "            # 使用指数加权移动平均\n",
    "            weights = np.exp(np.linspace(-1, 0, len(country_data)))\n",
    "            weighted_avg = np.average(country_data['Total'], weights=weights)\n",
    "            return weighted_avg\n",
    "        \n",
    "        emerging_metrics = []\n",
    "        max_year = data['Year'].max()\n",
    "        \n",
    "        for noc in data['NOC'].unique():\n",
    "            country_data = data[data['NOC'] == noc]\n",
    "            \n",
    "            # 分割时期\n",
    "            recent_data = country_data[country_data['Year'] >= (max_year - recent_years)]\n",
    "            early_data = country_data[country_data['Year'] < (max_year - recent_years)]\n",
    "            \n",
    "            if len(recent_data) < 2 or len(early_data) < 2:\n",
    "                continue\n",
    "                \n",
    "            # 计算复合指标\n",
    "            growth_score = calculate_growth_score(\n",
    "                early_data['Total'].mean(),\n",
    "                recent_data['Total'].mean()\n",
    "            )\n",
    "            \n",
    "            momentum_score = calculate_momentum_score(recent_data)\n",
    "            \n",
    "            # 计算稳定性\n",
    "            stability = 1 / (1 + recent_data['Total'].std())\n",
    "            \n",
    "            # 综合得分\n",
    "            final_score = (\n",
    "                growth_score * 0.4 +\n",
    "                momentum_score * 0.4 +\n",
    "                stability * 0.2\n",
    "            )\n",
    "            \n",
    "            if final_score > 0:\n",
    "                emerging_metrics.append({\n",
    "                    'NOC': noc,\n",
    "                    'growth_rate': float(growth_score),\n",
    "                    'momentum': float(momentum_score),\n",
    "                    'stability': float(stability),\n",
    "                    'final_score': float(final_score),\n",
    "                    'recent_medals': float(recent_data['Total'].mean()),\n",
    "                    'early_medals': float(early_data['Total'].mean())\n",
    "                })\n",
    "        \n",
    "        # 按综合得分排序\n",
    "        return sorted(emerging_metrics, key=lambda x: x['final_score'], reverse=True)[:10]\n",
    "    # def _identify_emerging_countries(self, data: pd.DataFrame, recent_years: int = 12) -> List[Dict]:\n",
    "    #     \"\"\"改进的新兴国家识别算法\"\"\"\n",
    "    #     # 数据预处理\n",
    "    #     historical_nocs = {'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC'}\n",
    "    #     data = data[~data['NOC'].isin(historical_nocs)].copy()\n",
    "        \n",
    "    #     # 计算时期表现\n",
    "    #     max_year = data['Year'].max()\n",
    "    #     recent_data = data[data['Year'] >= (max_year - recent_years)]\n",
    "    #     early_data = data[data['Year'] < (max_year - recent_years)]\n",
    "        \n",
    "    #     # 计算平均值\n",
    "    #     recent_avg = recent_data.groupby('NOC')['Total'].mean()\n",
    "    #     early_avg = early_data.groupby('NOC')['Total'].mean()\n",
    "        \n",
    "    #     # 计算增长\n",
    "    #     growth_df = pd.DataFrame({\n",
    "    #         'recent': recent_avg,\n",
    "    #         'early': early_avg\n",
    "    #     }).fillna(0)\n",
    "        \n",
    "    #     # 改进的增长率计算\n",
    "    #     growth_df['growth_rate'] = (\n",
    "    #         (growth_df['recent'] - growth_df['early']) / \n",
    "    #         (growth_df['early'].replace(0, 0.1))  # 避免除以0\n",
    "    #     ) * 100\n",
    "        \n",
    "    #     # 限制极端值\n",
    "    #     growth_df['growth_rate'] = growth_df['growth_rate'].clip(-1000, 1000)\n",
    "        \n",
    "    #     # 筛选条件\n",
    "    #     emerging = growth_df[\n",
    "    #         (growth_df['growth_rate'] > 20) &  # 显著增长\n",
    "    #         (growth_df['recent'] >= 3) &       # 当前有一定规模\n",
    "    #         (growth_df['recent'] > growth_df['early'])  # 确实在增长\n",
    "    #     ].sort_values('growth_rate', ascending=False)\n",
    "        \n",
    "    #     return [\n",
    "    #         {\n",
    "    #             'NOC': noc,\n",
    "    #             'growth_rate': float(emerging.loc[noc, 'growth_rate']),\n",
    "    #             'recent_medals': float(emerging.loc[noc, 'recent']),\n",
    "    #             'early_medals': float(growth_df.loc[noc, 'early'])\n",
    "    #         }\n",
    "    #         for noc in emerging.index[:10]\n",
    "    #     ]\n",
    "\n",
    "    def _identify_declining_countries(self, data: pd.DataFrame, recent_years: int = 12) -> List[Dict]:\n",
    "        \"\"\"改进的衰退国家识别\"\"\"\n",
    "        # 过滤历史国家\n",
    "        historical_nocs = {'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC', \n",
    "                        'Soviet Union', 'East Germany', 'West Germany', 'Unified Team'}\n",
    "        \n",
    "        data = data[~data['NOC'].isin(historical_nocs)].copy()\n",
    "        \n",
    "        # 计算各个时期的表现\n",
    "        recent_data = data[data['Year'] >= data['Year'].max() - recent_years]\n",
    "        early_data = data[data['Year'] < data['Year'].max() - recent_years]\n",
    "        \n",
    "        # 使用更复杂的衰退指标\n",
    "        decline_metrics = []\n",
    "        \n",
    "        for noc in data['NOC'].unique():\n",
    "            noc_recent = recent_data[recent_data['NOC'] == noc]\n",
    "            noc_early = early_data[early_data['NOC'] == noc]\n",
    "            \n",
    "            if len(noc_recent) < 2 or len(noc_early) < 2:\n",
    "                continue\n",
    "            \n",
    "            # 计算多个维度的衰退指标\n",
    "            medal_decline = (noc_early['Total'].mean() - noc_recent['Total'].mean()) / (noc_early['Total'].mean() + 1)\n",
    "            consistency_decline = noc_recent['Total'].std() / (noc_recent['Total'].mean() + 1) - \\\n",
    "                                noc_early['Total'].std() / (noc_early['Total'].mean() + 1)\n",
    "            peak_decline = noc_early['Total'].max() - noc_recent['Total'].max()\n",
    "            \n",
    "            # 仅考虑最近仍有参与的国家\n",
    "            if noc_recent['Year'].max() < data['Year'].max() - 4:\n",
    "                continue\n",
    "                \n",
    "            # 综合衰退分数\n",
    "            decline_score = (medal_decline * 0.5 + \n",
    "                            consistency_decline * 0.3 + \n",
    "                            (peak_decline / (noc_early['Total'].max() + 1)) * 0.2)\n",
    "            \n",
    "            if decline_score > 0.2:  # 显著衰退阈值\n",
    "                decline_metrics.append({\n",
    "                    'NOC': noc,\n",
    "                    'decline_rate': float(decline_score),\n",
    "                    'recent_medals': float(noc_recent['Total'].mean()),\n",
    "                    'early_medals': float(noc_early['Total'].mean()),\n",
    "                    'peak_medals': float(noc_early['Total'].max()),\n",
    "                    'last_appearance': int(noc_recent['Year'].max())\n",
    "                })\n",
    "        \n",
    "        return sorted(decline_metrics, key=lambda x: x['decline_rate'], reverse=True)[:10]\n",
    "\n",
    "    def _create_region_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"改进的区域映射，覆盖更多国家\"\"\"\n",
    "        region_mapping = {\n",
    "            # 北美洲\n",
    "            'USA': 'North America', 'CAN': 'North America', 'MEX': 'North America',\n",
    "            'PUR': 'North America', 'CUB': 'North America', 'JAM': 'North America',\n",
    "            # 南美洲\n",
    "            'BRA': 'South America', 'ARG': 'South America', 'CHI': 'South America',\n",
    "            'COL': 'South America', 'VEN': 'South America', 'PER': 'South America',\n",
    "            'ECU': 'South America', 'URU': 'South America', 'BOL': 'South America',\n",
    "            # 欧洲\n",
    "            'GBR': 'Europe', 'FRA': 'Europe', 'GER': 'Europe', 'ITA': 'Europe',\n",
    "            'ESP': 'Europe', 'NED': 'Europe', 'SWE': 'Europe', 'NOR': 'Europe',\n",
    "            'DEN': 'Europe', 'FIN': 'Europe', 'BEL': 'Europe', 'AUT': 'Europe',\n",
    "            'SUI': 'Europe', 'POL': 'Europe', 'HUN': 'Europe', 'CZE': 'Europe',\n",
    "            'GRE': 'Europe', 'UKR': 'Europe', 'RUS': 'Europe', 'TUR': 'Europe',\n",
    "            'POR': 'Europe', 'ROU': 'Europe', 'SRB': 'Europe', 'CRO': 'Europe',\n",
    "            # 亚洲\n",
    "            'CHN': 'Asia', 'JPN': 'Asia', 'KOR': 'Asia', 'PRK': 'Asia',\n",
    "            'IND': 'Asia', 'IRN': 'Asia', 'KAZ': 'Asia', 'THA': 'Asia',\n",
    "            'IDN': 'Asia', 'MAS': 'Asia', 'SGP': 'Asia', 'VNM': 'Asia',\n",
    "            'PHI': 'Asia', 'PAK': 'Asia', 'BAN': 'Asia', 'IRQ': 'Asia',\n",
    "            # 大洋洲\n",
    "            'AUS': 'Oceania', 'NZL': 'Oceania', 'FIJ': 'Oceania', 'PNG': 'Oceania',\n",
    "            # 非洲\n",
    "            'RSA': 'Africa', 'KEN': 'Africa', 'ETH': 'Africa', 'NGR': 'Africa',\n",
    "            'EGY': 'Africa', 'MAR': 'Africa', 'ALG': 'Africa', 'TUN': 'Africa',\n",
    "            'ZIM': 'Africa', 'UGA': 'Africa', 'GHA': 'Africa', 'CIV': 'Africa',\n",
    "            # 历史国家映射\n",
    "            'URS': 'Europe', 'GDR': 'Europe', 'FRG': 'Europe', 'TCH': 'Europe',\n",
    "            'YUG': 'Europe', 'EUN': 'Europe', 'ROC': 'Europe', 'SGP': 'Asia',\n",
    "        }\n",
    "        return region_mapping\n",
    "\n",
    "    def _analyze_regional_patterns(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"改进的区域分析，确保输出完整的区域数据\"\"\"\n",
    "        region_patterns = {}\n",
    "        regions = ['Europe', 'Asia', 'North America', 'South America', 'Africa', 'Oceania']\n",
    "        \n",
    "        # 确保区域映射\n",
    "        data['Region'] = data['NOC'].map(self._create_region_mapping())\n",
    "        data = data[data['Region'].isin(regions)]  # 只分析主要区域\n",
    "        \n",
    "        for region in regions:\n",
    "            region_data = data[data['Region'] == region]\n",
    "            if len(region_data) < 2:\n",
    "                continue\n",
    "                \n",
    "            # 计算区域指标\n",
    "            total_medals = region_data.groupby('Year')['Total'].sum()\n",
    "            \n",
    "            if len(total_medals) < 2:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # 趋势计算\n",
    "                years = np.array(total_medals.index).reshape(-1, 1)\n",
    "                medals = total_medals.values\n",
    "                trend = stats.theilslopes(medals, years)[0]\n",
    "                \n",
    "                # 近期表现\n",
    "                recent_years = data['Year'].max() - 8\n",
    "                recent_total = data[data['Year'] >= recent_years]['Total'].sum()\n",
    "                region_recent = region_data[region_data['Year'] >= recent_years]['Total'].sum()\n",
    "                recent_share = region_recent / recent_total if recent_total > 0 else 0\n",
    "                \n",
    "                # 主导国家\n",
    "                top_countries = region_data.groupby('NOC')['Total'].sum().nlargest(3)\n",
    "                \n",
    "                region_patterns[region] = {\n",
    "                    'trend': float(trend),\n",
    "                    'volatility': float(total_medals.std() / (total_medals.mean() + 1e-6)),\n",
    "                    'medal_median': float(total_medals.median()),\n",
    "                    'recent_share': float(recent_share),\n",
    "                    'top_countries': top_countries.index.tolist(),\n",
    "                    'total_countries': len(region_data['NOC'].unique())\n",
    "                }\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return region_patterns\n",
    "    def _calculate_robust_trend(self, series: pd.Series) -> float:\n",
    "        \"\"\"使用Theil-Sen回归计算稳健趋势\"\"\"\n",
    "        if len(series) < 2:\n",
    "            return 0.0\n",
    "        years = series.index.values.reshape(-1, 1)\n",
    "        medals = series.values\n",
    "        slope, _, _, _ = stats.theilslopes(medals, years)\n",
    "        return float(slope)\n",
    "    def _calculate_region_trend(self, region_data: pd.DataFrame) -> float:\n",
    "        \"\"\"使用Huber回归计算区域趋势\"\"\"\n",
    "        from sklearn.linear_model import HuberRegressor\n",
    "        X = region_data[['Year']].values\n",
    "        y = region_data['Total'].values\n",
    "        model = HuberRegressor().fit(X, y)\n",
    "        return float(model.coef_[0])\n",
    "    def _analyze_sport_diversity(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"改进的运动项目多样性分析，增加错误检查\"\"\"\n",
    "        try:\n",
    "            diversity = {}\n",
    "            \n",
    "            # 数据验证\n",
    "            if 'Sport' not in data.columns:\n",
    "                raise ValueError(\"Sport列不存在于数据中\")\n",
    "                \n",
    "            if 'NOC' not in data.columns:\n",
    "                raise ValueError(\"NOC列不存在于数据中\")\n",
    "                \n",
    "            # 打印整体统计信息\n",
    "            print(\"\\n运动项目多样性分析诊断:\")\n",
    "            print(f\"总记录数: {len(data)}\")\n",
    "            print(f\"唯一国家数: {data['NOC'].nunique()}\")\n",
    "            print(f\"唯一运动项目数: {data['Sport'].nunique()}\")\n",
    "            \n",
    "            # 计算每个国家在不同项目上的分布\n",
    "            country_sports = data.groupby('NOC')['Sport'].nunique()\n",
    "            print(f\"\\n国家运动项目分布:\")\n",
    "            print(f\"最大值: {country_sports.max()}\")\n",
    "            print(f\"最小值: {country_sports.min()}\")\n",
    "            print(f\"平均值: {country_sports.mean():.2f}\")\n",
    "            print(f\"中位数: {country_sports.median()}\")\n",
    "            \n",
    "            # 计算多样性指标\n",
    "            diversity['overall'] = {\n",
    "                'avg_sports': float(country_sports.mean()),\n",
    "                'max_sports': int(country_sports.max()),\n",
    "                'min_sports': int(country_sports.min()),\n",
    "                'median_sports': float(country_sports.median()),\n",
    "                'total_sports': len(data['Sport'].unique())\n",
    "            }\n",
    "            \n",
    "            # 识别专注型和多样化型国家\n",
    "            q25, q75 = country_sports.quantile([0.25, 0.75])\n",
    "            specialized = country_sports[country_sports < q25]\n",
    "            diversified = country_sports[country_sports > q75]\n",
    "            \n",
    "            print(f\"\\n专注型国家(低于25分位): {len(specialized)}\")\n",
    "            print(f\"多样化国家(高于75分位): {len(diversified)}\")\n",
    "            \n",
    "            diversity['specialized'] = specialized.to_dict()\n",
    "            diversity['diversified'] = diversified.to_dict()\n",
    "            \n",
    "            # 时间趋势分析\n",
    "            if 'Year' in data.columns:\n",
    "                recent_years = data['Year'].max() - 8\n",
    "                recent_data = data[data['Year'] >= recent_years]\n",
    "                recent_sports = recent_data.groupby('NOC')['Sport'].nunique()\n",
    "                \n",
    "                diversity['recent_trends'] = {\n",
    "                    'avg_sports_recent': float(recent_sports.mean()),\n",
    "                    'max_sports_recent': int(recent_sports.max()),\n",
    "                    'countries_increased': len(recent_sports[recent_sports > country_sports])\n",
    "                }\n",
    "            \n",
    "            return diversity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"错误: 分析运动项目多样性时发生异常: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return {\n",
    "                'overall': {'avg_sports': 0, 'max_sports': 0, 'min_sports': 0},\n",
    "                'specialized': {},\n",
    "                'diversified': {}\n",
    "            }\n",
    "\n",
    "    def generate_country_insights(self, medals_df: pd.DataFrame, athletes_df: pd.DataFrame) -> Dict[str, CountryInsight]:\n",
    "        insights = {}\n",
    "        \n",
    "        medals_df = medals_df.copy()\n",
    "        athletes_df = athletes_df.copy()\n",
    "\n",
    "        # 预处理：清理NOC代码中的额外空格\n",
    "        medals_df['NOC'] = medals_df['NOC'].str.strip()\n",
    "        \n",
    "        # NOC标准化映射\n",
    "        noc_mapping = {\n",
    "            'UNITED STATES': 'USA', 'GREAT BRITAIN': 'GBR', 'SOVIET UNION': 'URS',\n",
    "            'FRANCE': 'FRA', 'CHINA': 'CHN', 'GERMANY': 'GER', 'ITALY': 'ITA',\n",
    "            'AUSTRALIA': 'AUS', 'JAPAN': 'JPN', 'HUNGARY': 'HUN', 'SWEDEN': 'SWE',\n",
    "            'RUSSIA': 'RUS', 'EAST GERMANY': 'GDR', 'NETHERLANDS': 'NED',\n",
    "            'CANADA': 'CAN', 'SOUTH KOREA': 'KOR', 'ROMANIA': 'ROU', 'POLAND': 'POL',\n",
    "            'FINLAND': 'FIN', 'CUBA': 'CUB', 'BULGARIA': 'BUL', 'SWITZERLAND': 'SUI',\n",
    "            'WEST GERMANY': 'FRG', 'DENMARK': 'DEN', 'SPAIN': 'ESP', 'NORWAY': 'NOR',\n",
    "            'BRAZIL': 'BRA', 'BELGIUM': 'BEL', 'NEW ZEALAND': 'NZL'\n",
    "        }\n",
    "        \n",
    "        medals_df['NOC'] = medals_df['NOC'].apply(lambda x: noc_mapping.get(x.upper(), x))\n",
    "        \n",
    "        main_countries = medals_df.groupby('NOC')['Total'].sum().nlargest(30).index\n",
    "        \n",
    "        for noc in main_countries:\n",
    "            trend_score = self._calculate_trend_score(medals_df, noc)\n",
    "            stability_score = self._calculate_stability_score(medals_df, noc)\n",
    "            diversity_score = self._calculate_diversity_score(athletes_df, noc)\n",
    "            \n",
    "            key_findings = self._generate_key_findings(\n",
    "                medals_df, athletes_df, noc,\n",
    "                trend_score, stability_score, diversity_score\n",
    "            )\n",
    "            \n",
    "            recommendations = self._generate_recommendations(\n",
    "                key_findings, trend_score, stability_score, diversity_score\n",
    "            )\n",
    "            \n",
    "            insights[noc] = CountryInsight(\n",
    "                noc=noc,\n",
    "                trend_score=trend_score,\n",
    "                stability_score=stability_score,\n",
    "                diversity_score=diversity_score,\n",
    "                key_findings=key_findings,\n",
    "                recommendations=recommendations\n",
    "            )\n",
    "        \n",
    "        return insights\n",
    "    def _calculate_trend_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "        \"\"\"改进的趋势分析，使用Prophet模型捕捉非线性趋势\"\"\"\n",
    "        try:\n",
    "            from fbprophet import Prophet\n",
    "        except ImportError:\n",
    "            from prophet import Prophet\n",
    "        \n",
    "        country_data = data[data['NOC'] == noc].copy()\n",
    "        if len(country_data) < 4:  # 需要足够的数据点\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # 准备Prophet数据\n",
    "            df = pd.DataFrame({\n",
    "                'ds': pd.to_datetime(country_data['Year'].astype(str)),\n",
    "                'y': country_data['Total']\n",
    "            })\n",
    "            \n",
    "            # 拟合Prophet模型\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=False,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                interval_width=0.95\n",
    "            )\n",
    "            model.fit(df)\n",
    "            \n",
    "            # 预测未来趋势\n",
    "            future = model.make_future_dataframe(periods=1, freq='Y')\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # 计算趋势分数\n",
    "            trend = forecast['trend'].diff().mean()\n",
    "            \n",
    "            # 标准化趋势分数\n",
    "            max_trend = data.groupby('NOC')['Total'].mean().max()\n",
    "            normalized_trend = (trend + max_trend) / (2 * max_trend)\n",
    "            \n",
    "            # 考虑预测不确定性\n",
    "            uncertainty = (forecast['yhat_upper'] - forecast['yhat_lower']).mean()\n",
    "            uncertainty_penalty = 1 / (1 + uncertainty)\n",
    "            \n",
    "            final_score = normalized_trend * uncertainty_penalty\n",
    "            \n",
    "            return max(0.0, min(1.0, final_score))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Prophet analysis failed for {noc}: {str(e)}\")\n",
    "            # 降级为简单线性回归\n",
    "            return super()._calculate_trend_score(data, noc)\n",
    "    # def _calculate_trend_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "    #     \"\"\"计算国家的发展趋势得分\n",
    "    #     改进:\n",
    "    #     1. 考虑长期和短期趋势\n",
    "    #     2. 使用robust回归\n",
    "    #     3. 确保得分在0-1范围内\n",
    "    #     \"\"\"\n",
    "    #     country_data = data[data['NOC'] == noc].copy()\n",
    "    #     if len(country_data) < 2:\n",
    "    #         return 0.0\n",
    "        \n",
    "    #     # 准备数据\n",
    "    #     country_data = country_data.sort_values('Year')\n",
    "    #     years = (country_data['Year'] - country_data['Year'].min()).values\n",
    "    #     medals = country_data['Total'].values\n",
    "        \n",
    "    #     try:\n",
    "    #         # 使用RobustRegression避免异常值影响\n",
    "    #         regression = stats.theilslopes(medals, years)\n",
    "    #         trend = regression[0]  # 斜率\n",
    "            \n",
    "    #         # 计算近期趋势（最近3届）\n",
    "    #         recent_data = country_data.tail(3)\n",
    "    #         if len(recent_data) >= 2:\n",
    "    #             recent_years = (recent_data['Year'] - recent_data['Year'].min()).values\n",
    "    #             recent_medals = recent_data['Total'].values\n",
    "    #             recent_trend = stats.theilslopes(recent_medals, recent_years)[0]\n",
    "    #         else:\n",
    "    #             recent_trend = trend\n",
    "            \n",
    "    #         # 标准化趋势分数\n",
    "    #         all_trends = data.groupby('NOC').apply(\n",
    "    #             lambda x: stats.theilslopes(x['Total'].values, \n",
    "    #                                     (x['Year'] - x['Year'].min()).values)[0]\n",
    "    #             if len(x) >= 2 else 0\n",
    "    #         )\n",
    "            \n",
    "    #         # 使用百分位数进行标准化，确保分数在0-1之间\n",
    "    #         trend_percentile = stats.percentileofscore(all_trends, trend) / 100\n",
    "    #         recent_percentile = stats.percentileofscore(all_trends, recent_trend) / 100\n",
    "            \n",
    "    #         # 综合长期和近期趋势，近期趋势权重更大\n",
    "    #         final_score = trend_percentile * 0.4 + recent_percentile * 0.6\n",
    "            \n",
    "    #         return max(0.0, min(1.0, final_score))  # 确保在0-1范围内\n",
    "            \n",
    "    #     except Exception:\n",
    "    #         return 0.0\n",
    "\n",
    "    def _calculate_stability_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "        \"\"\"计算国家表现的稳定性得分\"\"\"\n",
    "        country_data = data[data['NOC'] == noc]\n",
    "        if len(country_data) < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        # 计算变异系数（标准差/平均值）\n",
    "        cv = country_data['Total'].std() / (country_data['Total'].mean() + 1e-6)\n",
    "        \n",
    "        # 转换为稳定性得分（越稳定越接近1）\n",
    "        return 1 / (1 + cv)\n",
    "\n",
    "    def _calculate_diversity_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "        \"\"\"改进的多样性得分计算\"\"\"\n",
    "        try:\n",
    "            # 规范化NOC处理\n",
    "            standardized_noc = noc.strip().upper()\n",
    "            country_data = data[data['NOC'] == standardized_noc].copy()\n",
    "            \n",
    "            if len(country_data) == 0:\n",
    "                print(f\"警告: {noc} (标准化后: {standardized_noc}) 没有运动项目数据\")\n",
    "                # 尝试模糊匹配\n",
    "                similar_nocs = data['NOC'].unique()\n",
    "                print(f\"数据中存在的相似NOC: {[n for n in similar_nocs if n.startswith(standardized_noc[:3])]}\")\n",
    "                return 0.0\n",
    "                \n",
    "            # 基本验证\n",
    "            if 'Sport' not in country_data.columns:\n",
    "                print(f\"错误: Sport列不存在\")\n",
    "                return 0.0\n",
    "                \n",
    "            # 数据统计\n",
    "            total_sports = len(data['Sport'].unique())\n",
    "            country_sports = len(country_data['Sport'].unique())\n",
    "            \n",
    "            # 1. 规模得分 (0-0.4)\n",
    "            scale_score = 0.4 * (country_sports / total_sports) if total_sports > 0 else 0\n",
    "            \n",
    "            # 2. 均衡度得分 (0-0.3)\n",
    "            sport_counts = country_data['Sport'].value_counts()\n",
    "            if len(sport_counts) > 1:\n",
    "                probs = sport_counts / sport_counts.sum()\n",
    "                entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "                max_entropy = np.log2(len(sport_counts))\n",
    "                balance_score = 0.3 * (entropy / max_entropy)\n",
    "            else:\n",
    "                balance_score = 0.0\n",
    "            \n",
    "            # 3. 参与度得分 (0-0.3)\n",
    "            recent_years = data['Year'].max() - 8\n",
    "            recent_data = country_data[country_data['Year'] >= recent_years]\n",
    "            participation_rate = len(recent_data['Sport'].unique()) / max(country_sports, 1)\n",
    "            participation_score = 0.3 * participation_rate\n",
    "            \n",
    "            final_score = scale_score + balance_score + participation_score\n",
    "            \n",
    "            return round(min(1.0, max(0.0, final_score)), 2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"错误: 计算{noc}的多样性得分时发生异常: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return 0.0\n",
    "\n",
    "    def _generate_key_findings(self, \n",
    "                             medals_df: pd.DataFrame, \n",
    "                             athletes_df: pd.DataFrame, \n",
    "                             noc: str,\n",
    "                             trend_score: float,\n",
    "                             stability_score: float,\n",
    "                             diversity_score: float) -> List[str]:\n",
    "        \"\"\"生成关键发现\"\"\"\n",
    "        findings = []\n",
    "        \n",
    "        # 分析趋势\n",
    "        if trend_score > 0.7:\n",
    "            findings.append(\"显示出强劲的上升势头\")\n",
    "        elif trend_score < 0.3:\n",
    "            findings.append(\"表现呈现下降趋势\")\n",
    "        \n",
    "        # 分析稳定性\n",
    "        if stability_score > 0.7:\n",
    "            findings.append(\"表现非常稳定\")\n",
    "        elif stability_score < 0.3:\n",
    "            findings.append(\"表现波动较大\")\n",
    "        \n",
    "        # 分析多样性\n",
    "        if diversity_score > 0.7:\n",
    "            findings.append(\"具有良好的项目多样性\")\n",
    "        elif diversity_score < 0.3:\n",
    "            findings.append(\"项目集中度较高\")\n",
    "        \n",
    "        return findings\n",
    "\n",
    "    def _generate_recommendations(self, findings: List[str],\n",
    "                                trend_score: float,\n",
    "                                stability_score: float,\n",
    "                                diversity_score: float) -> List[str]:\n",
    "        \"\"\"生成建议，保持原有接口\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # 基于趋势分数生成建议\n",
    "        if trend_score < 0.5:\n",
    "            recommendations.append(\"建议增加青少年训练营投入，重点发展潜力项目\")\n",
    "        elif trend_score > 0.8:\n",
    "            recommendations.append(\"可考虑在优势项目中建立长期统治地位\")\n",
    "        \n",
    "        # 基于稳定性分数生成建议\n",
    "        if stability_score < 0.5:\n",
    "            recommendations.append(\"需建立运动员伤病管理系统和后备人才库\")\n",
    "        elif stability_score < 0.7:\n",
    "            recommendations.append(\"加强运动员梯队建设，提高稳定性\")\n",
    "        \n",
    "        # 基于多样性分数生成建议\n",
    "        if diversity_score < 0.3:\n",
    "            recommendations.append(\"应优先拓展与现有优势项目相关的新分项\")\n",
    "        elif 0.3 <= diversity_score < 0.6:\n",
    "            recommendations.append(\"可尝试在新增奥运项目中寻找突破机会\")\n",
    "        elif diversity_score >= 0.6:\n",
    "            recommendations.append(\"保持项目多样性优势，巩固竞争实力\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def generate_report(self, trends: Dict, insights: Dict[str, CountryInsight]) -> str:\n",
    "        \"\"\"生成分析报告\"\"\"\n",
    "        report = []\n",
    "        \n",
    "        # 1. 总体趋势\n",
    "        report.append(\"1. 奥运会奖牌总体趋势分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        # 添加整体趋势分析\n",
    "        overall = trends.get('overall', {})\n",
    "        report.append(f\"\\n参与国家数量: {overall.get('total_countries', 'N/A')}\")\n",
    "        \n",
    "        # 添加奖牌集中度分析\n",
    "        concentration = overall.get('medals_concentration', {})\n",
    "        for period, stats in concentration.items():\n",
    "            report.append(f\"\\n{period}时期:\")\n",
    "            report.append(f\"  - 基尼系数: {stats['gini_coefficient']:.3f}\")\n",
    "            report.append(f\"  - 前10国家占比: {stats['top_10_share']*100:.1f}%\")\n",
    "        \n",
    "        # 2. 新兴与衰退趋势\n",
    "        report.append(\"\\n\\n2. 新兴与衰退国家分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        # 新兴国家\n",
    "        emerging = overall.get('emerging_countries', [])\n",
    "        report.append(\"\\n新兴奥运强国:\")\n",
    "        for country in emerging[:5]:\n",
    "            report.append(\n",
    "                f\"  - {country['NOC']}: 增长率 {country['growth_rate']*100:.1f}%, \"\n",
    "                f\"近期平均 {country['recent_medals']:.1f} 枚奖牌\"\n",
    "            )\n",
    "        \n",
    "        # 衰退国家\n",
    "        declining = overall.get('declining_countries', [])\n",
    "        report.append(\"\\n实力下降国家:\")\n",
    "        for country in declining[:5]:\n",
    "            report.append(\n",
    "                f\"  - {country['NOC']}: 下降率 {country['decline_rate']*100:.1f}%, \"\n",
    "                f\"近期平均 {country['recent_medals']:.1f} 枚奖牌\"\n",
    "            )\n",
    "        \n",
    "        # 3. 区域分析\n",
    "        report.append(\"\\n\\n3. 区域性分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        regional = trends.get('regional', {})\n",
    "        for region, stats in regional.items():\n",
    "            report.append(f\"\\n{region}:\")\n",
    "            report.append(f\"  - 趋势系数: {stats['trend']:.2f}\")\n",
    "            report.append(f\"  - 波动性: {stats['volatility']:.2f}\")\n",
    "            report.append(f\"  - 奖牌中位数: {stats['medal_median']:.1f}\")\n",
    "            report.append(f\"  - 近期份额: {stats['recent_share']*100:.1f}%\")\n",
    "        \n",
    "        # 4. 国家深度分析\n",
    "        report.append(\"\\n\\n4. 国家深度分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        for noc, insight in list(insights.items())[:10]:  # 展示前10个国家\n",
    "            report.append(f\"\\n{noc}分析:\")\n",
    "            report.append(f\"  趋势得分: {insight.trend_score:.2f}\")\n",
    "            report.append(f\"  稳定性得分: {insight.stability_score:.2f}\")\n",
    "            report.append(f\"  多样性得分: {insight.diversity_score:.2f}\")\n",
    "            report.append(\"  主要发现:\")\n",
    "            for finding in insight.key_findings:\n",
    "                report.append(f\"    - {finding}\")\n",
    "            report.append(\"  建议:\")\n",
    "            for recommendation in insight.recommendations:\n",
    "                report.append(f\"    - {recommendation}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "def main():\n",
    "    console = Console()\n",
    "    \n",
    "    try:\n",
    "        # 初始化分析器\n",
    "        analyzer = OlympicMedalInsightAnalyzer()\n",
    "        \n",
    "        # 加载数据\n",
    "        console.print(\"[bold cyan]加载数据...[/bold cyan]\")\n",
    "        medals_df, athletes_df, programs_df = analyzer.load_and_prepare_data()\n",
    "        \n",
    "        # 分析趋势\n",
    "        console.print(\"[bold cyan]分析奖牌趋势...[/bold cyan]\")\n",
    "        # 在 main() 函数中\n",
    "        trends = analyzer.analyze_medal_trends(medals_df, athletes_df)\n",
    "        \n",
    "        # 生成国家洞察\n",
    "        console.print(\"[bold cyan]生成国家洞察...[/bold cyan]\")\n",
    "        insights = analyzer.generate_country_insights(medals_df, athletes_df)\n",
    "        \n",
    "        # 生成报告\n",
    "        report = analyzer.generate_report(trends, insights)\n",
    "        \n",
    "        # 保存报告\n",
    "        output_dir = Path(\"analysis_results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_dir / \"olympic_medal_insights_report.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        # 显示报告\n",
    "        console.print(\"\\n[bold green]分析报告:[/bold green]\")\n",
    "        console.print(report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]错误: {str(e)}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class CountryInsight:\n",
    "    noc: str\n",
    "    trend_score: float\n",
    "    stability_score: float\n",
    "    diversity_score: float\n",
    "    key_findings: List[str]\n",
    "    recommendations: List[str]\n",
    "\n",
    "class OlympicMedalInsightAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.insights = {}\n",
    "        self.trends = {}\n",
    "        self.patterns = {}\n",
    "        \n",
    "    def load_and_prepare_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"加载和准备分析所需的数据\"\"\"\n",
    "        try:\n",
    "            def try_load_data(file_path_base: str) -> pd.DataFrame:\n",
    "                \"\"\"尝试多种方式加载数据\"\"\"\n",
    "                base_path = Path(file_path_base)\n",
    "                data_dir = base_path.parent\n",
    "                data_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                attempts = [\n",
    "                    (base_path.with_suffix('.parquet'), lambda x: pd.read_parquet(x)),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x)),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x, encoding='utf-8')),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x, encoding='latin1')),\n",
    "                    (base_path.parent / f\"{base_path.name}.parquet\", lambda x: pd.read_parquet(x)),\n",
    "                    (base_path.parent / f\"{base_path.name}.csv\", lambda x: pd.read_csv(x)),\n",
    "                    (base_path.parent / f\"{base_path.name}.xlsx\", lambda x: pd.read_excel(x))\n",
    "                ]\n",
    "                \n",
    "                errors = []\n",
    "                for file_path, reader in attempts:\n",
    "                    try:\n",
    "                        if file_path.exists():\n",
    "                            data = reader(file_path)\n",
    "                            self.console.print(f\"[green]成功从 {file_path} 加载数据[/green]\")\n",
    "                            return data\n",
    "                    except Exception as e:\n",
    "                        errors.append(f\"{file_path}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                available_files = list(data_dir.glob(\"*\")) if data_dir.exists() else []\n",
    "                files_str = \"\\n\".join(f\"- {f.name}\" for f in available_files) if available_files else \"目录为空\"\n",
    "                \n",
    "                error_msg = (\n",
    "                    f\"无法加载数据文件 {base_path}.*\\n\"\n",
    "                    f\"尝试的路径:\\n{chr(10).join(f'- {err}' for err in errors)}\\n\"\n",
    "                    f\"目录 {data_dir} 中的文件:\\n{files_str}\"\n",
    "                )\n",
    "                raise FileNotFoundError(error_msg)\n",
    "\n",
    "            # 加载数据\n",
    "            medals_df = try_load_data(\"data/processed/medal_counts\")\n",
    "            athletes_df = try_load_data(\"data/processed/athletes\")\n",
    "            programs_df = try_load_data(\"data/processed/programs\")\n",
    "            \n",
    "            # 数据预处理\n",
    "            for df in [medals_df, athletes_df, programs_df]:\n",
    "                if 'Year' in df.columns:\n",
    "                    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "                    \n",
    "            # 移除重复项\n",
    "            medals_df = medals_df.drop_duplicates()\n",
    "            athletes_df = athletes_df.drop_duplicates()\n",
    "            programs_df = programs_df.drop_duplicates()\n",
    "            \n",
    "            # 验证必要的列\n",
    "            required_cols = {\n",
    "                'medals_df': ['Year', 'NOC', 'Gold', 'Total'],\n",
    "                'athletes_df': ['Year', 'NOC', 'Sport'],\n",
    "                'programs_df': ['Sport', 'Discipline']\n",
    "            }\n",
    "            \n",
    "            for df_name, df in [\n",
    "                ('medals_df', medals_df), \n",
    "                ('athletes_df', athletes_df), \n",
    "                ('programs_df', programs_df)\n",
    "            ]:\n",
    "                missing_cols = [col for col in required_cols[df_name] if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f\"{df_name} 缺少必要的列: {', '.join(missing_cols)}\")\n",
    "            \n",
    "            return medals_df, athletes_df, programs_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]数据加载错误: {str(e)}[/bold red]\")\n",
    "            raise\n",
    "\n",
    "    def analyze_medal_trends(self, medals_df: pd.DataFrame, athletes_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"分析奖牌趋势和模式\"\"\"\n",
    "        # 预处理：过滤历史国家\n",
    "        historical_nocs = {\n",
    "            'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC',\n",
    "            'Soviet Union', 'East Germany', 'West Germany', 'Unified Team'\n",
    "        }\n",
    "        medals_df = medals_df[~medals_df['NOC'].isin(historical_nocs)].copy()\n",
    "        athletes_df = athletes_df[~athletes_df['NOC'].isin(historical_nocs)].copy()\n",
    "        trends = {}\n",
    "        \n",
    "        # 1. 整体趋势分析\n",
    "        overall_trends = {\n",
    "            'total_countries': medals_df['NOC'].nunique(),\n",
    "            'medals_concentration': self._calculate_medals_concentration(medals_df),\n",
    "            'emerging_countries': self._identify_emerging_countries(medals_df),\n",
    "            'declining_countries': self._identify_declining_countries(medals_df)\n",
    "        }\n",
    "        trends['overall'] = overall_trends\n",
    "        \n",
    "        # 2. 区域性分析\n",
    "        region_mapping = self._create_region_mapping()\n",
    "        medals_df['Region'] = medals_df['NOC'].map(region_mapping)\n",
    "        regional_trends = self._analyze_regional_patterns(medals_df)\n",
    "        trends['regional'] = regional_trends\n",
    "        \n",
    "        # 3. 项目多样性分析 - 使用 athletes_df 而不是 medals_df\n",
    "        diversity_trends = self._analyze_sport_diversity(athletes_df)\n",
    "        trends['diversity'] = diversity_trends\n",
    "        \n",
    "        return trends\n",
    "\n",
    "    def _calculate_medals_concentration(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"计算奖牌集中度趋势\"\"\"\n",
    "        concentration = {}\n",
    "        \n",
    "        # 按时期分析\n",
    "        periods = [(1896, 1950), (1951, 2000), (2001, 2024)]\n",
    "        \n",
    "        for start, end in periods:\n",
    "            period_data = data[(data['Year'] >= start) & (data['Year'] <= end)]\n",
    "            total_medals = period_data.groupby('NOC')['Total'].sum()\n",
    "            \n",
    "            # 计算基尼系数\n",
    "            gini = self._calculate_gini(total_medals.values)\n",
    "            \n",
    "            # 计算前10国家占比\n",
    "            top_10_share = total_medals.nlargest(10).sum() / total_medals.sum()\n",
    "            \n",
    "            concentration[f\"{start}-{end}\"] = {\n",
    "                'gini_coefficient': gini,\n",
    "                'top_10_share': top_10_share,\n",
    "                'total_countries': len(total_medals)\n",
    "            }\n",
    "            \n",
    "        return concentration\n",
    "\n",
    "    def _calculate_gini(self, array: np.ndarray) -> float:\n",
    "        \"\"\"计算基尼系数\"\"\"\n",
    "        array = array.flatten()\n",
    "        if len(array) == 0:\n",
    "            return 0\n",
    "        array = np.sort(array)\n",
    "        index = np.arange(1, len(array) + 1)\n",
    "        n = len(array)\n",
    "        return ((2 * index - n - 1) * array).sum() / (n * array.sum())\n",
    "    def _identify_emerging_countries(self, data: pd.DataFrame, recent_years: int = 12) -> List[Dict]:\n",
    "        \"\"\"改进的新兴国家识别算法，使用复合指标\"\"\"\n",
    "        historical_nocs = {'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC'}\n",
    "        data = data[~data['NOC'].isin(historical_nocs)].copy()\n",
    "        \n",
    "        def calculate_growth_score(early_medals: float, recent_medals: float) -> float:\n",
    "            \"\"\"使用对数增长率避免除零问题\"\"\"\n",
    "            if early_medals == 0:\n",
    "                early_medals = 0.5  # Laplace平滑\n",
    "            return np.log((recent_medals + 1) / (early_medals + 1))\n",
    "        \n",
    "        def calculate_momentum_score(country_data: pd.DataFrame) -> float:\n",
    "            \"\"\"计算动量得分\"\"\"\n",
    "            if len(country_data) < 3:\n",
    "                return 0\n",
    "            \n",
    "            # 使用指数加权移动平均\n",
    "            weights = np.exp(np.linspace(-1, 0, len(country_data)))\n",
    "            weighted_avg = np.average(country_data['Total'], weights=weights)\n",
    "            return weighted_avg\n",
    "        \n",
    "        emerging_metrics = []\n",
    "        max_year = data['Year'].max()\n",
    "        \n",
    "        for noc in data['NOC'].unique():\n",
    "            country_data = data[data['NOC'] == noc]\n",
    "            \n",
    "            # 分割时期\n",
    "            recent_data = country_data[country_data['Year'] >= (max_year - recent_years)]\n",
    "            early_data = country_data[country_data['Year'] < (max_year - recent_years)]\n",
    "            \n",
    "            if len(recent_data) < 2 or len(early_data) < 2:\n",
    "                continue\n",
    "                \n",
    "            # 计算复合指标\n",
    "            growth_score = calculate_growth_score(\n",
    "                early_data['Total'].mean(),\n",
    "                recent_data['Total'].mean()\n",
    "            )\n",
    "            \n",
    "            momentum_score = calculate_momentum_score(recent_data)\n",
    "            \n",
    "            # 计算稳定性\n",
    "            stability = 1 / (1 + recent_data['Total'].std())\n",
    "            \n",
    "            # 综合得分\n",
    "            final_score = (\n",
    "                growth_score * 0.4 +\n",
    "                momentum_score * 0.4 +\n",
    "                stability * 0.2\n",
    "            )\n",
    "            \n",
    "            if final_score > 0:\n",
    "                emerging_metrics.append({\n",
    "                    'NOC': noc,\n",
    "                    'growth_rate': float(growth_score),\n",
    "                    'momentum': float(momentum_score),\n",
    "                    'stability': float(stability),\n",
    "                    'final_score': float(final_score),\n",
    "                    'recent_medals': float(recent_data['Total'].mean()),\n",
    "                    'early_medals': float(early_data['Total'].mean())\n",
    "                })\n",
    "        \n",
    "        # 按综合得分排序\n",
    "        return sorted(emerging_metrics, key=lambda x: x['final_score'], reverse=True)[:10]\n",
    "    # def _identify_emerging_countries(self, data: pd.DataFrame, recent_years: int = 12) -> List[Dict]:\n",
    "    #     \"\"\"改进的新兴国家识别算法\"\"\"\n",
    "    #     # 数据预处理\n",
    "    #     historical_nocs = {'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC'}\n",
    "    #     data = data[~data['NOC'].isin(historical_nocs)].copy()\n",
    "        \n",
    "    #     # 计算时期表现\n",
    "    #     max_year = data['Year'].max()\n",
    "    #     recent_data = data[data['Year'] >= (max_year - recent_years)]\n",
    "    #     early_data = data[data['Year'] < (max_year - recent_years)]\n",
    "        \n",
    "    #     # 计算平均值\n",
    "    #     recent_avg = recent_data.groupby('NOC')['Total'].mean()\n",
    "    #     early_avg = early_data.groupby('NOC')['Total'].mean()\n",
    "        \n",
    "    #     # 计算增长\n",
    "    #     growth_df = pd.DataFrame({\n",
    "    #         'recent': recent_avg,\n",
    "    #         'early': early_avg\n",
    "    #     }).fillna(0)\n",
    "        \n",
    "    #     # 改进的增长率计算\n",
    "    #     growth_df['growth_rate'] = (\n",
    "    #         (growth_df['recent'] - growth_df['early']) / \n",
    "    #         (growth_df['early'].replace(0, 0.1))  # 避免除以0\n",
    "    #     ) * 100\n",
    "        \n",
    "    #     # 限制极端值\n",
    "    #     growth_df['growth_rate'] = growth_df['growth_rate'].clip(-1000, 1000)\n",
    "        \n",
    "    #     # 筛选条件\n",
    "    #     emerging = growth_df[\n",
    "    #         (growth_df['growth_rate'] > 20) &  # 显著增长\n",
    "    #         (growth_df['recent'] >= 3) &       # 当前有一定规模\n",
    "    #         (growth_df['recent'] > growth_df['early'])  # 确实在增长\n",
    "    #     ].sort_values('growth_rate', ascending=False)\n",
    "        \n",
    "    #     return [\n",
    "    #         {\n",
    "    #             'NOC': noc,\n",
    "    #             'growth_rate': float(emerging.loc[noc, 'growth_rate']),\n",
    "    #             'recent_medals': float(emerging.loc[noc, 'recent']),\n",
    "    #             'early_medals': float(growth_df.loc[noc, 'early'])\n",
    "    #         }\n",
    "    #         for noc in emerging.index[:10]\n",
    "    #     ]\n",
    "\n",
    "    def _identify_declining_countries(self, data: pd.DataFrame, recent_years: int = 12) -> List[Dict]:\n",
    "        \"\"\"改进的衰退国家识别\"\"\"\n",
    "        # 过滤历史国家\n",
    "        historical_nocs = {'URS', 'GDR', 'FRG', 'EUN', 'YUG', 'TCH', 'ROC', \n",
    "                        'Soviet Union', 'East Germany', 'West Germany', 'Unified Team'}\n",
    "        \n",
    "        data = data[~data['NOC'].isin(historical_nocs)].copy()\n",
    "        \n",
    "        # 计算各个时期的表现\n",
    "        recent_data = data[data['Year'] >= data['Year'].max() - recent_years]\n",
    "        early_data = data[data['Year'] < data['Year'].max() - recent_years]\n",
    "        \n",
    "        # 使用更复杂的衰退指标\n",
    "        decline_metrics = []\n",
    "        \n",
    "        for noc in data['NOC'].unique():\n",
    "            noc_recent = recent_data[recent_data['NOC'] == noc]\n",
    "            noc_early = early_data[early_data['NOC'] == noc]\n",
    "            \n",
    "            if len(noc_recent) < 2 or len(noc_early) < 2:\n",
    "                continue\n",
    "            \n",
    "            # 计算多个维度的衰退指标\n",
    "            medal_decline = (noc_early['Total'].mean() - noc_recent['Total'].mean()) / (noc_early['Total'].mean() + 1)\n",
    "            consistency_decline = noc_recent['Total'].std() / (noc_recent['Total'].mean() + 1) - \\\n",
    "                                noc_early['Total'].std() / (noc_early['Total'].mean() + 1)\n",
    "            peak_decline = noc_early['Total'].max() - noc_recent['Total'].max()\n",
    "            \n",
    "            # 仅考虑最近仍有参与的国家\n",
    "            if noc_recent['Year'].max() < data['Year'].max() - 4:\n",
    "                continue\n",
    "                \n",
    "            # 综合衰退分数\n",
    "            decline_score = (medal_decline * 0.5 + \n",
    "                            consistency_decline * 0.3 + \n",
    "                            (peak_decline / (noc_early['Total'].max() + 1)) * 0.2)\n",
    "            \n",
    "            if decline_score > 0.2:  # 显著衰退阈值\n",
    "                decline_metrics.append({\n",
    "                    'NOC': noc,\n",
    "                    'decline_rate': float(decline_score),\n",
    "                    'recent_medals': float(noc_recent['Total'].mean()),\n",
    "                    'early_medals': float(noc_early['Total'].mean()),\n",
    "                    'peak_medals': float(noc_early['Total'].max()),\n",
    "                    'last_appearance': int(noc_recent['Year'].max())\n",
    "                })\n",
    "        \n",
    "        return sorted(decline_metrics, key=lambda x: x['decline_rate'], reverse=True)[:10]\n",
    "\n",
    "    def _create_region_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"改进的区域映射，覆盖更多国家\"\"\"\n",
    "        region_mapping = {\n",
    "            # 北美洲\n",
    "            'USA': 'North America', 'CAN': 'North America', 'MEX': 'North America',\n",
    "            'PUR': 'North America', 'CUB': 'North America', 'JAM': 'North America',\n",
    "            # 南美洲\n",
    "            'BRA': 'South America', 'ARG': 'South America', 'CHI': 'South America',\n",
    "            'COL': 'South America', 'VEN': 'South America', 'PER': 'South America',\n",
    "            'ECU': 'South America', 'URU': 'South America', 'BOL': 'South America',\n",
    "            # 欧洲\n",
    "            'GBR': 'Europe', 'FRA': 'Europe', 'GER': 'Europe', 'ITA': 'Europe',\n",
    "            'ESP': 'Europe', 'NED': 'Europe', 'SWE': 'Europe', 'NOR': 'Europe',\n",
    "            'DEN': 'Europe', 'FIN': 'Europe', 'BEL': 'Europe', 'AUT': 'Europe',\n",
    "            'SUI': 'Europe', 'POL': 'Europe', 'HUN': 'Europe', 'CZE': 'Europe',\n",
    "            'GRE': 'Europe', 'UKR': 'Europe', 'RUS': 'Europe', 'TUR': 'Europe',\n",
    "            'POR': 'Europe', 'ROU': 'Europe', 'SRB': 'Europe', 'CRO': 'Europe',\n",
    "            # 亚洲\n",
    "            'CHN': 'Asia', 'JPN': 'Asia', 'KOR': 'Asia', 'PRK': 'Asia',\n",
    "            'IND': 'Asia', 'IRN': 'Asia', 'KAZ': 'Asia', 'THA': 'Asia',\n",
    "            'IDN': 'Asia', 'MAS': 'Asia', 'SGP': 'Asia', 'VNM': 'Asia',\n",
    "            'PHI': 'Asia', 'PAK': 'Asia', 'BAN': 'Asia', 'IRQ': 'Asia',\n",
    "            # 大洋洲\n",
    "            'AUS': 'Oceania', 'NZL': 'Oceania', 'FIJ': 'Oceania', 'PNG': 'Oceania',\n",
    "            # 非洲\n",
    "            'RSA': 'Africa', 'KEN': 'Africa', 'ETH': 'Africa', 'NGR': 'Africa',\n",
    "            'EGY': 'Africa', 'MAR': 'Africa', 'ALG': 'Africa', 'TUN': 'Africa',\n",
    "            'ZIM': 'Africa', 'UGA': 'Africa', 'GHA': 'Africa', 'CIV': 'Africa',\n",
    "            # 历史国家映射\n",
    "            'URS': 'Europe', 'GDR': 'Europe', 'FRG': 'Europe', 'TCH': 'Europe',\n",
    "            'YUG': 'Europe', 'EUN': 'Europe', 'ROC': 'Europe', 'SGP': 'Asia',\n",
    "        }\n",
    "        return region_mapping\n",
    "\n",
    "    def _analyze_regional_patterns(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Enhanced regional analysis with comprehensive metrics\n",
    "        \"\"\"\n",
    "        region_patterns = {}\n",
    "        regions = ['Europe', 'Asia', 'North America', 'South America', 'Africa', 'Oceania']\n",
    "        \n",
    "        # Ensure region mapping\n",
    "        data['Region'] = data['NOC'].map(self._create_region_mapping())\n",
    "        data = data[data['Region'].isin(regions)]\n",
    "        \n",
    "        max_year = data['Year'].max()\n",
    "        recent_cutoff = max_year - 12\n",
    "        \n",
    "        for region in regions:\n",
    "            region_data = data[data['Region'] == region]\n",
    "            if len(region_data) < 2:\n",
    "                continue\n",
    "                \n",
    "            recent_data = region_data[region_data['Year'] > recent_cutoff]\n",
    "            historical_data = region_data[region_data['Year'] <= recent_cutoff]\n",
    "            \n",
    "            try:\n",
    "                # Calculate comprehensive metrics\n",
    "                recent_medals = recent_data.groupby('Year')['Total'].sum().mean()\n",
    "                historical_medals = historical_data.groupby('Year')['Total'].sum().mean()\n",
    "                \n",
    "                growth_rate = ((recent_medals - historical_medals) / \n",
    "                            (historical_medals + 1e-6))\n",
    "                \n",
    "                # Calculate region dominance\n",
    "                total_recent = data[data['Year'] > recent_cutoff]['Total'].sum()\n",
    "                region_recent = recent_data['Total'].sum()\n",
    "                region_share = region_recent / total_recent if total_recent > 0 else 0\n",
    "                \n",
    "                # Top performing countries\n",
    "                top_countries = recent_data.groupby('NOC')['Total'].sum().nlargest(3)\n",
    "                \n",
    "                # Diversity of strong performers\n",
    "                countries_above_threshold = len(recent_data.groupby('NOC')['Total'].sum()[\n",
    "                    lambda x: x > x.mean()\n",
    "                ])\n",
    "                \n",
    "                region_patterns[region] = {\n",
    "                    'trend': float(growth_rate),\n",
    "                    'recent_medals_avg': float(recent_medals),\n",
    "                    'historical_medals_avg': float(historical_medals),\n",
    "                    'region_share': float(region_share),\n",
    "                    'top_countries': top_countries.index.tolist(),\n",
    "                    'strong_performers_count': countries_above_threshold,\n",
    "                    'total_countries': len(region_data['NOC'].unique())\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {region}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return region_patterns\n",
    "    def _calculate_robust_trend(self, series: pd.Series) -> float:\n",
    "        \"\"\"使用Theil-Sen回归计算稳健趋势\"\"\"\n",
    "        if len(series) < 2:\n",
    "            return 0.0\n",
    "        years = series.index.values.reshape(-1, 1)\n",
    "        medals = series.values\n",
    "        slope, _, _, _ = stats.theilslopes(medals, years)\n",
    "        return float(slope)\n",
    "    def _calculate_region_trend(self, region_data: pd.DataFrame) -> float:\n",
    "        \"\"\"使用Huber回归计算区域趋势\"\"\"\n",
    "        from sklearn.linear_model import HuberRegressor\n",
    "        X = region_data[['Year']].values\n",
    "        y = region_data['Total'].values\n",
    "        model = HuberRegressor().fit(X, y)\n",
    "        return float(model.coef_[0])\n",
    "    def _analyze_sport_diversity(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"改进的运动项目多样性分析，增加错误检查\"\"\"\n",
    "        try:\n",
    "            diversity = {}\n",
    "            \n",
    "            # 数据验证\n",
    "            if 'Sport' not in data.columns:\n",
    "                raise ValueError(\"Sport列不存在于数据中\")\n",
    "                \n",
    "            if 'NOC' not in data.columns:\n",
    "                raise ValueError(\"NOC列不存在于数据中\")\n",
    "                \n",
    "            # 打印整体统计信息\n",
    "            print(\"\\n运动项目多样性分析诊断:\")\n",
    "            print(f\"总记录数: {len(data)}\")\n",
    "            print(f\"唯一国家数: {data['NOC'].nunique()}\")\n",
    "            print(f\"唯一运动项目数: {data['Sport'].nunique()}\")\n",
    "            \n",
    "            # 计算每个国家在不同项目上的分布\n",
    "            country_sports = data.groupby('NOC')['Sport'].nunique()\n",
    "            print(f\"\\n国家运动项目分布:\")\n",
    "            print(f\"最大值: {country_sports.max()}\")\n",
    "            print(f\"最小值: {country_sports.min()}\")\n",
    "            print(f\"平均值: {country_sports.mean():.2f}\")\n",
    "            print(f\"中位数: {country_sports.median()}\")\n",
    "            \n",
    "            # 计算多样性指标\n",
    "            diversity['overall'] = {\n",
    "                'avg_sports': float(country_sports.mean()),\n",
    "                'max_sports': int(country_sports.max()),\n",
    "                'min_sports': int(country_sports.min()),\n",
    "                'median_sports': float(country_sports.median()),\n",
    "                'total_sports': len(data['Sport'].unique())\n",
    "            }\n",
    "            \n",
    "            # 识别专注型和多样化型国家\n",
    "            q25, q75 = country_sports.quantile([0.25, 0.75])\n",
    "            specialized = country_sports[country_sports < q25]\n",
    "            diversified = country_sports[country_sports > q75]\n",
    "            \n",
    "            print(f\"\\n专注型国家(低于25分位): {len(specialized)}\")\n",
    "            print(f\"多样化国家(高于75分位): {len(diversified)}\")\n",
    "            \n",
    "            diversity['specialized'] = specialized.to_dict()\n",
    "            diversity['diversified'] = diversified.to_dict()\n",
    "            \n",
    "            # 时间趋势分析\n",
    "            if 'Year' in data.columns:\n",
    "                recent_years = data['Year'].max() - 8\n",
    "                recent_data = data[data['Year'] >= recent_years]\n",
    "                recent_sports = recent_data.groupby('NOC')['Sport'].nunique()\n",
    "                \n",
    "                diversity['recent_trends'] = {\n",
    "                    'avg_sports_recent': float(recent_sports.mean()),\n",
    "                    'max_sports_recent': int(recent_sports.max()),\n",
    "                    'countries_increased': len(recent_sports[recent_sports > country_sports])\n",
    "                }\n",
    "            \n",
    "            return diversity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"错误: 分析运动项目多样性时发生异常: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return {\n",
    "                'overall': {'avg_sports': 0, 'max_sports': 0, 'min_sports': 0},\n",
    "                'specialized': {},\n",
    "                'diversified': {}\n",
    "            }\n",
    "\n",
    "    def generate_country_insights(self, medals_df: pd.DataFrame, athletes_df: pd.DataFrame) -> Dict[str, CountryInsight]:\n",
    "        insights = {}\n",
    "        \n",
    "        medals_df = medals_df.copy()\n",
    "        athletes_df = athletes_df.copy()\n",
    "\n",
    "        # 预处理：清理NOC代码中的额外空格\n",
    "        medals_df['NOC'] = medals_df['NOC'].str.strip()\n",
    "        \n",
    "        # NOC标准化映射\n",
    "        noc_mapping = {\n",
    "            'UNITED STATES': 'USA', 'GREAT BRITAIN': 'GBR', 'SOVIET UNION': 'URS',\n",
    "            'FRANCE': 'FRA', 'CHINA': 'CHN', 'GERMANY': 'GER', 'ITALY': 'ITA',\n",
    "            'AUSTRALIA': 'AUS', 'JAPAN': 'JPN', 'HUNGARY': 'HUN', 'SWEDEN': 'SWE',\n",
    "            'RUSSIA': 'RUS', 'EAST GERMANY': 'GDR', 'NETHERLANDS': 'NED',\n",
    "            'CANADA': 'CAN', 'SOUTH KOREA': 'KOR', 'ROMANIA': 'ROU', 'POLAND': 'POL',\n",
    "            'FINLAND': 'FIN', 'CUBA': 'CUB', 'BULGARIA': 'BUL', 'SWITZERLAND': 'SUI',\n",
    "            'WEST GERMANY': 'FRG', 'DENMARK': 'DEN', 'SPAIN': 'ESP', 'NORWAY': 'NOR',\n",
    "            'BRAZIL': 'BRA', 'BELGIUM': 'BEL', 'NEW ZEALAND': 'NZL'\n",
    "        }\n",
    "        \n",
    "        medals_df['NOC'] = medals_df['NOC'].apply(lambda x: noc_mapping.get(x.upper(), x))\n",
    "        \n",
    "        main_countries = medals_df.groupby('NOC')['Total'].sum().nlargest(30).index\n",
    "        \n",
    "        for noc in main_countries:\n",
    "            trend_score = self._calculate_trend_score(medals_df, noc)\n",
    "            stability_score = self._calculate_stability_score(medals_df, noc)\n",
    "            diversity_score = self._calculate_diversity_score(athletes_df, noc)\n",
    "            \n",
    "            key_findings = self._generate_key_findings(\n",
    "                medals_df, athletes_df, noc,\n",
    "                trend_score, stability_score, diversity_score\n",
    "            )\n",
    "            \n",
    "            recommendations = self._generate_recommendations(\n",
    "                key_findings, trend_score, stability_score, diversity_score\n",
    "            )\n",
    "            \n",
    "            insights[noc] = CountryInsight(\n",
    "                noc=noc,\n",
    "                trend_score=trend_score,\n",
    "                stability_score=stability_score,\n",
    "                diversity_score=diversity_score,\n",
    "                key_findings=key_findings,\n",
    "                recommendations=recommendations\n",
    "            )\n",
    "        \n",
    "        return insights\n",
    "    def _calculate_trend_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate trend score using weighted recent performance and robust regression\n",
    "        \"\"\"\n",
    "        country_data = data[data['NOC'] == noc].copy()\n",
    "        if len(country_data) < 4:\n",
    "            return 0.0\n",
    "            \n",
    "        try:\n",
    "            # Split data into recent and historical periods\n",
    "            max_year = country_data['Year'].max()\n",
    "            recent_cutoff = max_year - 12  # Last 3 Olympics\n",
    "            \n",
    "            recent_data = country_data[country_data['Year'] > recent_cutoff]\n",
    "            historical_data = country_data[country_data['Year'] <= recent_cutoff]\n",
    "            \n",
    "            # Calculate trends for both periods using Theil-Sen estimator\n",
    "            def calc_period_trend(df):\n",
    "                if len(df) < 2:\n",
    "                    return 0\n",
    "                years = (df['Year'] - df['Year'].min()).values\n",
    "                medals = df['Total'].values\n",
    "                return stats.theilslopes(medals, years)[0]\n",
    "                \n",
    "            recent_trend = calc_period_trend(recent_data)\n",
    "            historical_trend = calc_period_trend(historical_data)\n",
    "            \n",
    "            # Calculate relative performance\n",
    "            recent_avg = recent_data['Total'].mean() if len(recent_data) > 0 else 0\n",
    "            historical_avg = historical_data['Total'].mean() if len(historical_data) > 0 else 0\n",
    "            relative_change = (recent_avg - historical_avg) / (historical_avg + 1)\n",
    "            \n",
    "            # Combine metrics with weights\n",
    "            trend_score = (\n",
    "                0.5 * (recent_trend / (abs(recent_trend) + 1)) +  # Recent trend (normalized)\n",
    "                0.3 * (historical_trend / (abs(historical_trend) + 1)) +  # Historical trend\n",
    "                0.2 * (relative_change)  # Overall improvement\n",
    "            )\n",
    "            \n",
    "            # Normalize to 0-1 range\n",
    "            return max(0.0, min(1.0, (trend_score + 1) / 2))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating trend score for {noc}: {str(e)}\")\n",
    "            return 0.0\n",
    "    # def _calculate_trend_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "    #     \"\"\"计算国家的发展趋势得分\n",
    "    #     改进:\n",
    "    #     1. 考虑长期和短期趋势\n",
    "    #     2. 使用robust回归\n",
    "    #     3. 确保得分在0-1范围内\n",
    "    #     \"\"\"\n",
    "    #     country_data = data[data['NOC'] == noc].copy()\n",
    "    #     if len(country_data) < 2:\n",
    "    #         return 0.0\n",
    "        \n",
    "    #     # 准备数据\n",
    "    #     country_data = country_data.sort_values('Year')\n",
    "    #     years = (country_data['Year'] - country_data['Year'].min()).values\n",
    "    #     medals = country_data['Total'].values\n",
    "        \n",
    "    #     try:\n",
    "    #         # 使用RobustRegression避免异常值影响\n",
    "    #         regression = stats.theilslopes(medals, years)\n",
    "    #         trend = regression[0]  # 斜率\n",
    "            \n",
    "    #         # 计算近期趋势（最近3届）\n",
    "    #         recent_data = country_data.tail(3)\n",
    "    #         if len(recent_data) >= 2:\n",
    "    #             recent_years = (recent_data['Year'] - recent_data['Year'].min()).values\n",
    "    #             recent_medals = recent_data['Total'].values\n",
    "    #             recent_trend = stats.theilslopes(recent_medals, recent_years)[0]\n",
    "    #         else:\n",
    "    #             recent_trend = trend\n",
    "            \n",
    "    #         # 标准化趋势分数\n",
    "    #         all_trends = data.groupby('NOC').apply(\n",
    "    #             lambda x: stats.theilslopes(x['Total'].values, \n",
    "    #                                     (x['Year'] - x['Year'].min()).values)[0]\n",
    "    #             if len(x) >= 2 else 0\n",
    "    #         )\n",
    "            \n",
    "    #         # 使用百分位数进行标准化，确保分数在0-1之间\n",
    "    #         trend_percentile = stats.percentileofscore(all_trends, trend) / 100\n",
    "    #         recent_percentile = stats.percentileofscore(all_trends, recent_trend) / 100\n",
    "            \n",
    "    #         # 综合长期和近期趋势，近期趋势权重更大\n",
    "    #         final_score = trend_percentile * 0.4 + recent_percentile * 0.6\n",
    "            \n",
    "    #         return max(0.0, min(1.0, final_score))  # 确保在0-1范围内\n",
    "            \n",
    "    #     except Exception:\n",
    "    #         return 0.0\n",
    "\n",
    "    def _calculate_stability_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "        \"\"\"计算国家表现的稳定性得分\"\"\"\n",
    "        country_data = data[data['NOC'] == noc]\n",
    "        if len(country_data) < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        # 计算变异系数（标准差/平均值）\n",
    "        cv = country_data['Total'].std() / (country_data['Total'].mean() + 1e-6)\n",
    "        \n",
    "        # 转换为稳定性得分（越稳定越接近1）\n",
    "        return 1 / (1 + cv)\n",
    "\n",
    "    def _calculate_diversity_score(self, data: pd.DataFrame, noc: str) -> float:\n",
    "        \"\"\"改进的多样性得分计算\"\"\"\n",
    "        try:\n",
    "            # 规范化NOC处理\n",
    "            standardized_noc = noc.strip().upper()\n",
    "            country_data = data[data['NOC'] == standardized_noc].copy()\n",
    "            \n",
    "            if len(country_data) == 0:\n",
    "                print(f\"警告: {noc} (标准化后: {standardized_noc}) 没有运动项目数据\")\n",
    "                # 尝试模糊匹配\n",
    "                similar_nocs = data['NOC'].unique()\n",
    "                print(f\"数据中存在的相似NOC: {[n for n in similar_nocs if n.startswith(standardized_noc[:3])]}\")\n",
    "                return 0.0\n",
    "                \n",
    "            # 基本验证\n",
    "            if 'Sport' not in country_data.columns:\n",
    "                print(f\"错误: Sport列不存在\")\n",
    "                return 0.0\n",
    "                \n",
    "            # 数据统计\n",
    "            total_sports = len(data['Sport'].unique())\n",
    "            country_sports = len(country_data['Sport'].unique())\n",
    "            \n",
    "            # 1. 规模得分 (0-0.4)\n",
    "            scale_score = 0.4 * (country_sports / total_sports) if total_sports > 0 else 0\n",
    "            \n",
    "            # 2. 均衡度得分 (0-0.3)\n",
    "            sport_counts = country_data['Sport'].value_counts()\n",
    "            if len(sport_counts) > 1:\n",
    "                probs = sport_counts / sport_counts.sum()\n",
    "                entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "                max_entropy = np.log2(len(sport_counts))\n",
    "                balance_score = 0.3 * (entropy / max_entropy)\n",
    "            else:\n",
    "                balance_score = 0.0\n",
    "            \n",
    "            # 3. 参与度得分 (0-0.3)\n",
    "            recent_years = data['Year'].max() - 8\n",
    "            recent_data = country_data[country_data['Year'] >= recent_years]\n",
    "            participation_rate = len(recent_data['Sport'].unique()) / max(country_sports, 1)\n",
    "            participation_score = 0.3 * participation_rate\n",
    "            \n",
    "            final_score = scale_score + balance_score + participation_score\n",
    "            \n",
    "            return round(min(1.0, max(0.0, final_score)), 2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"错误: 计算{noc}的多样性得分时发生异常: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return 0.0\n",
    "\n",
    "    def _generate_key_findings(self, \n",
    "                             medals_df: pd.DataFrame, \n",
    "                             athletes_df: pd.DataFrame, \n",
    "                             noc: str,\n",
    "                             trend_score: float,\n",
    "                             stability_score: float,\n",
    "                             diversity_score: float) -> List[str]:\n",
    "        \"\"\"生成关键发现\"\"\"\n",
    "        findings = []\n",
    "        \n",
    "        # 分析趋势\n",
    "        if trend_score > 0.7:\n",
    "            findings.append(\"显示出强劲的上升势头\")\n",
    "        elif trend_score < 0.3:\n",
    "            findings.append(\"表现呈现下降趋势\")\n",
    "        \n",
    "        # 分析稳定性\n",
    "        if stability_score > 0.7:\n",
    "            findings.append(\"表现非常稳定\")\n",
    "        elif stability_score < 0.3:\n",
    "            findings.append(\"表现波动较大\")\n",
    "        \n",
    "        # 分析多样性\n",
    "        if diversity_score > 0.7:\n",
    "            findings.append(\"具有良好的项目多样性\")\n",
    "        elif diversity_score < 0.3:\n",
    "            findings.append(\"项目集中度较高\")\n",
    "        \n",
    "        return findings\n",
    "\n",
    "    def _generate_recommendations(self, findings: List[str],\n",
    "                                trend_score: float,\n",
    "                                stability_score: float,\n",
    "                                diversity_score: float) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate targeted recommendations based on comprehensive analysis\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Trend-based recommendations\n",
    "        if trend_score < 0.3:\n",
    "            recommendations.append(\"建议加大重点项目投入，制定中长期人才培养计划\")\n",
    "        elif trend_score < 0.6:\n",
    "            recommendations.append(\"保持现有优势项目投入，同时开拓新的潜力项目\")\n",
    "        else:\n",
    "            recommendations.append(\"巩固优势项目领先地位，建立可持续的竞技体系\")\n",
    "        \n",
    "        # Stability-based recommendations\n",
    "        if stability_score < 0.4:\n",
    "            recommendations.append(\"加强后备人才梯队建设，建立科学的选材体系\")\n",
    "        elif stability_score < 0.7:\n",
    "            recommendations.append(\"优化训练体系，提高竞技水平稳定性\")\n",
    "        else:\n",
    "            recommendations.append(\"完善人才储备机制，保持成绩稳定性\")\n",
    "        \n",
    "        # Diversity-based recommendations\n",
    "        if diversity_score < 0.3:\n",
    "            recommendations.append(\"拓展优势项目相关新项目，培养复合型人才\")\n",
    "        elif diversity_score < 0.6:\n",
    "            recommendations.append(\"在保持优势项目的同时，积极开发新的竞技项目\")\n",
    "        else:\n",
    "            recommendations.append(\"维持项目多样性优势，优化资源分配策略\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def generate_report(self, trends: Dict, insights: Dict[str, CountryInsight]) -> str:\n",
    "        \"\"\"生成分析报告\"\"\"\n",
    "        report = []\n",
    "        \n",
    "        # 1. 总体趋势\n",
    "        report.append(\"1. 奥运会奖牌总体趋势分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        # 添加整体趋势分析\n",
    "        overall = trends.get('overall', {})\n",
    "        report.append(f\"\\n参与国家数量: {overall.get('total_countries', 'N/A')}\")\n",
    "        \n",
    "        # 添加奖牌集中度分析\n",
    "        concentration = overall.get('medals_concentration', {})\n",
    "        for period, stats in concentration.items():\n",
    "            report.append(f\"\\n{period}时期:\")\n",
    "            report.append(f\"  - 基尼系数: {stats['gini_coefficient']:.3f}\")\n",
    "            report.append(f\"  - 前10国家占比: {stats['top_10_share']*100:.1f}%\")\n",
    "        \n",
    "        # 2. 新兴与衰退趋势\n",
    "        report.append(\"\\n\\n2. 新兴与衰退国家分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        # 新兴国家\n",
    "        emerging = overall.get('emerging_countries', [])\n",
    "        report.append(\"\\n新兴奥运强国:\")\n",
    "        for country in emerging[:5]:\n",
    "            report.append(\n",
    "                f\"  - {country['NOC']}: 增长率 {country['growth_rate']*100:.1f}%, \"\n",
    "                f\"近期平均 {country['recent_medals']:.1f} 枚奖牌\"\n",
    "            )\n",
    "        \n",
    "        # 衰退国家\n",
    "        declining = overall.get('declining_countries', [])\n",
    "        report.append(\"\\n实力下降国家:\")\n",
    "        for country in declining[:5]:\n",
    "            report.append(\n",
    "                f\"  - {country['NOC']}: 下降率 {country['decline_rate']*100:.1f}%, \"\n",
    "                f\"近期平均 {country['recent_medals']:.1f} 枚奖牌\"\n",
    "            )\n",
    "        \n",
    "        # 3. 区域分析\n",
    "        report.append(\"\\n\\n3. 区域性分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        regional = trends.get('regional', {})\n",
    "        for region, stats in regional.items():\n",
    "            report.append(f\"\\n{region}:\")\n",
    "            report.append(f\"  - 趋势系数: {stats['trend']:.2f}\")\n",
    "            report.append(f\"  - 波动性: {stats['volatility']:.2f}\")\n",
    "            report.append(f\"  - 奖牌中位数: {stats['medal_median']:.1f}\")\n",
    "            report.append(f\"  - 近期份额: {stats['recent_share']*100:.1f}%\")\n",
    "        \n",
    "        # 4. 国家深度分析\n",
    "        report.append(\"\\n\\n4. 国家深度分析\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        for noc, insight in list(insights.items())[:10]:  # 展示前10个国家\n",
    "            report.append(f\"\\n{noc}分析:\")\n",
    "            report.append(f\"  趋势得分: {insight.trend_score:.2f}\")\n",
    "            report.append(f\"  稳定性得分: {insight.stability_score:.2f}\")\n",
    "            report.append(f\"  多样性得分: {insight.diversity_score:.2f}\")\n",
    "            report.append(\"  主要发现:\")\n",
    "            for finding in insight.key_findings:\n",
    "                report.append(f\"    - {finding}\")\n",
    "            report.append(\"  建议:\")\n",
    "            for recommendation in insight.recommendations:\n",
    "                report.append(f\"    - {recommendation}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "def main():\n",
    "    console = Console()\n",
    "    \n",
    "    try:\n",
    "        # 初始化分析器\n",
    "        analyzer = OlympicMedalInsightAnalyzer()\n",
    "        \n",
    "        # 加载数据\n",
    "        console.print(\"[bold cyan]加载数据...[/bold cyan]\")\n",
    "        medals_df, athletes_df, programs_df = analyzer.load_and_prepare_data()\n",
    "        \n",
    "        # 分析趋势\n",
    "        console.print(\"[bold cyan]分析奖牌趋势...[/bold cyan]\")\n",
    "        # 在 main() 函数中\n",
    "        trends = analyzer.analyze_medal_trends(medals_df, athletes_df)\n",
    "        \n",
    "        # 生成国家洞察\n",
    "        console.print(\"[bold cyan]生成国家洞察...[/bold cyan]\")\n",
    "        insights = analyzer.generate_country_insights(medals_df, athletes_df)\n",
    "        \n",
    "        # 生成报告\n",
    "        report = analyzer.generate_report(trends, insights)\n",
    "        \n",
    "        # 保存报告\n",
    "        output_dir = Path(\"analysis_results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_dir / \"olympic_medal_insights_report.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        # 显示报告\n",
    "        console.print(\"\\n[bold green]分析报告:[/bold green]\")\n",
    "        console.print(report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]错误: {str(e)}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def _find_advanced_benchmark(self, data: pd.DataFrame, sport: str, target_country: str) -> Dict:\n",
    "        \"\"\"Improved benchmark country selection with dynamic window and decay\"\"\"\n",
    "\n",
    "        def calculate_sport_strength(country_data: pd.DataFrame, window_years: int = 8) -> float:\n",
    "            \"\"\"Calculate sport strength with temporal weighting\"\"\"\n",
    "            if len(country_data) < 3:\n",
    "                return 0.0\n",
    "\n",
    "            recent_data = country_data.sort_values('Year', ascending=False).head(window_years)\n",
    "            if recent_data.empty:\n",
    "                return 0.0\n",
    "\n",
    "            # Exponential decay weights\n",
    "            years = recent_data['Year'].values\n",
    "            max_year = years.max()\n",
    "            weights = np.exp(-0.2 * (max_year - years))\n",
    "\n",
    "            # Weighted metrics\n",
    "            weighted_medals = np.average(recent_data['Medal_Value'].values, weights=weights)\n",
    "            weighted_trend = np.polyfit(range(len(recent_data)), recent_data['Medal_Value'].values, 1, w=weights)[0]\n",
    "\n",
    "            return 0.7 * weighted_medals + 0.3 * weighted_trend\n",
    "\n",
    "        def calculate_historical_fit(country_data: pd.DataFrame) -> float:\n",
    "            \"\"\"Calculate historical fit score with peak performance consideration\"\"\"\n",
    "            if len(country_data) < 3:\n",
    "                return 0.0\n",
    "\n",
    "            years = country_data['Year'].values\n",
    "            medals = country_data['Medal_Value'].values\n",
    "\n",
    "            # Historical peak\n",
    "            peak_medals = np.percentile(medals, 95)\n",
    "\n",
    "            # Recent performance (last 3 cycles)\n",
    "            recent_mask = years >= (years.max() - 12)\n",
    "            recent_medals = medals[recent_mask] if any(recent_mask) else medals\n",
    "\n",
    "            if len(recent_medals) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            recent_avg = np.mean(recent_medals)\n",
    "\n",
    "            # Consistency factor\n",
    "            consistency = 1 / (1 + np.std(recent_medals))\n",
    "\n",
    "            # Combined score\n",
    "            historical_fit = (0.4 * (recent_avg / peak_medals) +\n",
    "                              0.4 * consistency +\n",
    "                              0.2 * (len(country_data) / 20))  # Experience factor\n",
    "\n",
    "            return float(np.clip(historical_fit, 0, 1))\n",
    "\n",
    "        try:\n",
    "            # Dynamic analysis period based on sport\n",
    "            sport_cycles = {\n",
    "                'Swimming': 8,\n",
    "                'Athletics': 12,\n",
    "                'Gymnastics': 16,\n",
    "                'Volleyball': 8\n",
    "            }\n",
    "            analysis_period = sport_cycles.get(sport, 12)\n",
    "\n",
    "            # Country code normalization\n",
    "            country_mapping = {\n",
    "                'URS': 'RUS', 'GDR': 'GER', 'FRG': 'GER',\n",
    "                'TCH': 'CZE', 'YUG': 'SRB'\n",
    "            }\n",
    "            normalized_country = country_mapping.get(target_country, target_country)\n",
    "\n",
    "            # Get target country data\n",
    "            target_data = data[\n",
    "                (data['Sport'] == sport) &\n",
    "                (data['Year'] >= data['Year'].max() - analysis_period) &\n",
    "                (data['NOC'] == normalized_country)\n",
    "                ]\n",
    "\n",
    "            if target_data.empty:\n",
    "                return None\n",
    "\n",
    "            # Calculate target metrics\n",
    "            target_strength = calculate_sport_strength(target_data)\n",
    "            target_fit = calculate_historical_fit(target_data)\n",
    "\n",
    "            # Calculate current performance with decay\n",
    "            recent_performance = target_data.sort_values('Year', ascending=False)['Medal_Value'].head(3).mean()\n",
    "            if recent_performance == 0:\n",
    "                # Find last medal year and apply decay\n",
    "                last_medal = data[\n",
    "                    (data['Sport'] == sport) &\n",
    "                    (data['NOC'] == normalized_country) &\n",
    "                    (data['Medal_Value'] > 0)\n",
    "                    ].sort_values('Year', ascending=False)\n",
    "\n",
    "                if not last_medal.empty:\n",
    "                    years_since = data['Year'].max() - last_medal['Year'].iloc[0]\n",
    "                    decay_factor = 0.8 ** (years_since / 4)  # 20% decay per cycle\n",
    "                    recent_performance = last_medal['Medal_Value'].iloc[0] * decay_factor\n",
    "\n",
    "            # Find benchmark countries\n",
    "            other_countries = []\n",
    "            for country in data[data['NOC'] != normalized_country]['NOC'].unique():\n",
    "                if country in country_mapping.keys():  # Skip historical countries\n",
    "                    continue\n",
    "\n",
    "                country_data = data[\n",
    "                    (data['Sport'] == sport) &\n",
    "                    (data['Year'] >= data['Year'].max() - analysis_period) &\n",
    "                    (data['NOC'] == country)\n",
    "                    ]\n",
    "\n",
    "                if len(country_data) >= 3:\n",
    "                    strength = calculate_sport_strength(country_data)\n",
    "                    hist_fit = calculate_historical_fit(country_data)\n",
    "\n",
    "                    if strength > target_strength:\n",
    "                        performance = country_data.sort_values('Year', ascending=False)['Medal_Value'].head(3).mean()\n",
    "\n",
    "                        # Monte Carlo simulation for uncertainty\n",
    "                        volatility = {'Swimming': 0.15, 'Gymnastics': 0.25}.get(sport, 0.2)\n",
    "                        simulations = np.random.normal(performance, performance * volatility, 1000)\n",
    "                        ci = np.percentile(simulations, [5, 95])\n",
    "\n",
    "                        other_countries.append({\n",
    "                            'NOC': country,\n",
    "                            'strength': strength,\n",
    "                            'historical_fit': hist_fit,\n",
    "                            'performance': performance,\n",
    "                            'uncertainty': (ci[0], ci[1]),\n",
    "                            'score': 0.4 * strength + 0.3 * hist_fit + 0.3 * (performance / (target_strength + 1e-6))\n",
    "                        })\n",
    "\n",
    "            if not other_countries:\n",
    "                return None\n",
    "\n",
    "            # Select best benchmark\n",
    "            best_benchmark = max(other_countries, key=lambda x: x['score'])\n",
    "\n",
    "            # Calculate normalized improvement potential\n",
    "            sport_max = data.groupby('Sport')['Medal_Value'].max()[sport]\n",
    "            improvement_potential = (best_benchmark['performance'] - recent_performance) / (sport_max + 1e-6)\n",
    "\n",
    "            return {\n",
    "                'sport': sport,\n",
    "                'benchmark_country': str(best_benchmark['NOC']),\n",
    "                'current_performance': float(recent_performance),\n",
    "                'improvement_potential': float(np.clip(improvement_potential, 0, 1)),\n",
    "                'historical_fit': float(target_fit),\n",
    "                'estimated_medal_gain': {\n",
    "                    'mean': float(best_benchmark['performance'] - recent_performance),\n",
    "                    'range': f\"{best_benchmark['uncertainty'][0]:.1f}-{best_benchmark['uncertainty'][1]:.1f}\"\n",
    "                },\n",
    "                'benchmark_metrics': {\n",
    "                    'avg_performance': float(best_benchmark['performance']),\n",
    "                    'stability': float(best_benchmark['historical_fit']),\n",
    "                    'experience_years': int(len(target_data))\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Benchmark analysis error: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def recommend_coach_investments(self, athletes_df: pd.DataFrame, medals_df: pd.DataFrame,\n",
    "                                    countries: List[str]) -> Dict:\n",
    "        \"\"\"增强版教练投资建议系统\"\"\"\n",
    "\n",
    "        def analyze_historical_data(data: pd.DataFrame, country: str, sport: str) -> Dict:\n",
    "            \"\"\"分析历史数据并计算关键指标\"\"\"\n",
    "            print(f\"\\n分析 {country} 在 {sport} 项目的历史数据:\")\n",
    "\n",
    "            # 检查原始数据\n",
    "            print(f\"数据集中unique的Sport值: {data['Sport'].unique()}\")\n",
    "            print(f\"数据集中unique的NOC值: {data['NOC'].unique()}\")\n",
    "\n",
    "            # 放宽匹配条件，使用模糊匹配\n",
    "            sport_mask = data['Sport'].str.contains(sport, case=False, na=False)\n",
    "            country_mask = data['NOC'].str.contains(country, case=False, na=False)\n",
    "\n",
    "            country_data = data[sport_mask & country_mask].copy()\n",
    "\n",
    "            print(f\"找到 {len(country_data)} 条原始数据记录\")\n",
    "            if len(country_data) == 0:\n",
    "                print(f\"尝试更广泛的搜索...\")\n",
    "                # 尝试查找可能的运动项目名称变体\n",
    "                possible_sports = data[data['Sport'].str.contains(sport[:4], case=False, na=False)]['Sport'].unique()\n",
    "                print(f\"可能的运动项目: {possible_sports}\")\n",
    "\n",
    "                # 尝试查找可能的国家代码变体\n",
    "                possible_countries = data[data['NOC'].str.contains(country[:2], case=False, na=False)]['NOC'].unique()\n",
    "                print(f\"可能的国家代码: {possible_countries}\")\n",
    "\n",
    "                # 使用更宽松的匹配\n",
    "                sport_mask = data['Sport'].isin(possible_sports)\n",
    "                country_mask = data['NOC'].isin(possible_countries)\n",
    "                country_data = data[sport_mask & country_mask].copy()\n",
    "                print(f\"宽松匹配后找到 {len(country_data)} 条记录\")\n",
    "\n",
    "            if len(country_data) == 0:\n",
    "                print(f\"警告: {country} 在 {sport} 项目中没有数据\")\n",
    "                return create_empty_metrics()\n",
    "\n",
    "            # 验证数据的完整性\n",
    "            print(\"\\n数据验证:\")\n",
    "            print(f\"年份范围: {country_data['Year'].min()} - {country_data['Year'].max()}\")\n",
    "            print(f\"Medal_Value统计: \\n{country_data['Medal_Value'].describe()}\")\n",
    "\n",
    "            # 计算年度统计\n",
    "            yearly_medals = country_data.groupby('Year')['Medal_Value'].sum()\n",
    "            print(f\"\\n年度奖牌统计:\\n{yearly_medals}\")\n",
    "\n",
    "            # 处理异常值\n",
    "            q1, q3 = yearly_medals.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            yearly_medals = yearly_medals.clip(upper=upper_bound)\n",
    "\n",
    "            # 计算近期表现 (最近8年)\n",
    "            recent_years = yearly_medals.index >= yearly_medals.index.max() - 8\n",
    "            recent_performance = yearly_medals[recent_years].mean() if any(recent_years) else 0\n",
    "            print(f\"近期平均表现: {recent_performance}\")\n",
    "\n",
    "            # 计算趋势\n",
    "            if len(yearly_medals) >= 2:\n",
    "                years = np.array(range(len(yearly_medals)))\n",
    "                trend = np.polyfit(years, yearly_medals.values, 1)[0]\n",
    "            else:\n",
    "                trend = 0\n",
    "            print(f\"趋势系数: {trend}\")\n",
    "\n",
    "            # 计算历史适配度指标\n",
    "            total_medals = yearly_medals.sum()\n",
    "            n_years = len(yearly_medals)\n",
    "            span_years = yearly_medals.index.max() - yearly_medals.index.min() + 4\n",
    "            participation_rate = n_years * 4 / span_years if span_years > 0 else 0\n",
    "\n",
    "            # 计算稳定性\n",
    "            consistency = 1 / (1 + yearly_medals.std()) if len(yearly_medals) > 1 else 0\n",
    "\n",
    "            print(f\"\"\"\n",
    "        详细统计:\n",
    "        - 总奖牌数: {total_medals}\n",
    "        - 参赛年数: {n_years}\n",
    "        - 参与率: {participation_rate:.2f}\n",
    "        - 稳定性: {consistency:.2f}\n",
    "        \"\"\")\n",
    "\n",
    "            # 综合历史适配度\n",
    "            historical_fit = (\n",
    "                    0.4 * (total_medals / (n_years + 1)) / 10 +  # 归一化平均奖牌数\n",
    "                    0.3 * participation_rate +\n",
    "                    0.3 * consistency\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                'peak': float(yearly_medals.max()),\n",
    "                'recent': float(recent_performance),\n",
    "                'trend': float(trend),\n",
    "                'years': int(n_years),\n",
    "                'historical_fit': float(historical_fit),\n",
    "                'consistency': float(consistency),\n",
    "                'total_medals': int(total_medals)\n",
    "            }\n",
    "\n",
    "            print(f\"最终指标:\\n{metrics}\")\n",
    "            return metrics\n",
    "\n",
    "        def create_empty_metrics():\n",
    "            \"\"\"创建空指标\"\"\"\n",
    "            return {\n",
    "                'peak': 0,\n",
    "                'recent': 0,\n",
    "                'trend': 0,\n",
    "                'years': 0,\n",
    "                'historical_fit': 0,\n",
    "                'consistency': 0,\n",
    "                'total_medals': 0\n",
    "            }\n",
    "\n",
    "        def evaluate_improvement_potential(target: Dict, benchmark: Dict) -> float:\n",
    "            \"\"\"评估改进潜力\"\"\"\n",
    "            print(f\"\\n评估改进潜力:\")\n",
    "            print(f\"目标指标: {target}\")\n",
    "            print(f\"标杆指标: {benchmark}\")\n",
    "\n",
    "            # 如果是新项目或历史表现较弱\n",
    "            if target['total_medals'] < 3:\n",
    "                base_potential = 0.3\n",
    "                print(f\"新项目或历史较弱，基础潜力: {base_potential}\")\n",
    "                return base_potential\n",
    "\n",
    "            # 计算相对差距\n",
    "            relative_gap = (benchmark['recent'] - target['recent']) / (benchmark['recent'] + 1)\n",
    "            print(f\"相对差距: {relative_gap}\")\n",
    "\n",
    "            # 趋势因子\n",
    "            trend_factor = np.tanh(max(0, benchmark['trend'] - target['trend']))\n",
    "            print(f\"趋势因子: {trend_factor}\")\n",
    "\n",
    "            # 历史因子\n",
    "            historical_factor = benchmark['historical_fit'] / (target['historical_fit'] + 0.1)\n",
    "            print(f\"历史因子: {historical_factor}\")\n",
    "\n",
    "            # 综合评分\n",
    "            potential = (\n",
    "                    0.5 * relative_gap +\n",
    "                    0.3 * trend_factor +\n",
    "                    0.2 * min(historical_factor, 2)  # 限制历史因子的影响\n",
    "            )\n",
    "\n",
    "            final_potential = float(np.clip(potential, 0, 1))\n",
    "            print(f\"最终潜力评分: {final_potential}\")\n",
    "            return final_potential\n",
    "\n",
    "        try:\n",
    "            recommendations = {}\n",
    "            print(f\"\\n开始生成教练投资建议...\")\n",
    "\n",
    "            # 基础运动项目\n",
    "            sports = ['Swimming', 'Athletics', 'Gymnastics', 'Volleyball']\n",
    "\n",
    "            # 合理的最大奖牌预期\n",
    "            max_medals = {\n",
    "                'Swimming': 15,\n",
    "                'Athletics': 12,\n",
    "                'Gymnastics': 10,\n",
    "                'Volleyball': 8\n",
    "            }\n",
    "\n",
    "            for country in countries:\n",
    "                print(f\"\\n分析 {country} 的投资机会:\")\n",
    "                country_recommendations = []\n",
    "\n",
    "                for sport in sports:\n",
    "                    print(f\"\\n评估 {sport} 项目:\")\n",
    "                    try:\n",
    "                        # 获取目标国家数据\n",
    "                        target_metrics = analyze_historical_data(athletes_df, country, sport)\n",
    "\n",
    "                        # 获取标杆国家数据\n",
    "                        benchmark_data = athletes_df[\n",
    "                            (athletes_df['Sport'] == sport) &\n",
    "                            (athletes_df['NOC'] != country) &\n",
    "                            (~athletes_df['NOC'].isin(['URS', 'GDR', 'FRG', 'TCH', 'YUG']))\n",
    "                            ]\n",
    "\n",
    "                        # 选择最佳表现的国家作为标杆\n",
    "                        benchmark_country = benchmark_data.groupby('NOC')['Medal_Value'].sum().nlargest(1).index[0]\n",
    "                        print(f\"选择的标杆国家: {benchmark_country}\")\n",
    "\n",
    "                        benchmark_metrics = analyze_historical_data(athletes_df, benchmark_country, sport)\n",
    "\n",
    "                        # 计算潜力\n",
    "                        potential = evaluate_improvement_potential(target_metrics, benchmark_metrics)\n",
    "\n",
    "                        # 计算预期奖牌增长\n",
    "                        medal_gain = min(\n",
    "                            max_medals[sport],\n",
    "                            benchmark_metrics['recent'] - target_metrics['recent']\n",
    "                        )\n",
    "\n",
    "                        if potential >= 0.1:  # 降低潜力阈值\n",
    "                            country_recommendations.append({\n",
    "                                'sport': sport,\n",
    "                                'benchmark_country': benchmark_country,\n",
    "                                'current_performance': float(target_metrics['recent']),\n",
    "                                'improvement_potential': float(potential),\n",
    "                                'historical_fit': float(target_metrics['historical_fit']),\n",
    "                                'estimated_medal_gain': {\n",
    "                                    'mean': float(medal_gain),\n",
    "                                    'range': f\"{(medal_gain * 0.85):.1f}-{(medal_gain * 1.15):.1f}\"\n",
    "                                },\n",
    "                                'benchmark_metrics': {\n",
    "                                    'avg_performance': float(benchmark_metrics['recent']),\n",
    "                                    'trend': float(benchmark_metrics['trend']),\n",
    "                                    'experience_years': int(benchmark_metrics['years'])\n",
    "                                }\n",
    "                            })\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理 {sport} 时出错: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                recommendations[country] = sorted(\n",
    "                    country_recommendations,\n",
    "                    key=lambda x: x['improvement_potential'],\n",
    "                    reverse=True\n",
    "                )\n",
    "\n",
    "            return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "\n",
    "@dataclass\n",
    "class CoachImpact:\n",
    "    sport: str\n",
    "    country: str\n",
    "    period: Tuple[int, int]\n",
    "    medal_change: float\n",
    "    significance: float\n",
    "    consistency: float\n",
    "\n",
    "class CoachImpactAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.coach_effects = {}\n",
    "        self.country_recommendations = {}\n",
    "\n",
    "    def load_and_prepare_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"加载和准备分析所需的数据\"\"\"\n",
    "        try:\n",
    "            def try_load_data(file_path_base):\n",
    "                \"\"\"尝试多种方式加载数据\"\"\"\n",
    "                # 确保基础路径是Path对象\n",
    "                base_path = Path(file_path_base)\n",
    "                data_dir = base_path.parent\n",
    "                \n",
    "                # 如果目录不存在，尝试创建\n",
    "                data_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # 定义加载尝试列表\n",
    "                attempts = [\n",
    "                    (base_path.with_suffix('.parquet'), lambda x: pd.read_parquet(x)),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x)),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x, encoding='utf-8')),\n",
    "                    (base_path.with_suffix('.csv'), lambda x: pd.read_csv(x, encoding='latin1')),\n",
    "                    (base_path.parent / f\"{base_path.name}.parquet\", lambda x: pd.read_parquet(x)),\n",
    "                    (base_path.parent / f\"{base_path.name}.csv\", lambda x: pd.read_csv(x))\n",
    "                ]\n",
    "                \n",
    "                errors = []\n",
    "                for file_path, reader in attempts:\n",
    "                    try:\n",
    "                        if file_path.exists():\n",
    "                            data = reader(file_path)\n",
    "                            self.console.print(f\"[green]成功从 {file_path} 加载数据[/green]\")\n",
    "                            return data\n",
    "                    except Exception as e:\n",
    "                        errors.append(f\"{file_path}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # 检查目录内容\n",
    "                available_files = list(data_dir.glob(\"*\")) if data_dir.exists() else []\n",
    "                files_str = \"\\n\".join(f\"- {f.name}\" for f in available_files) if available_files else \"目录为空\"\n",
    "                \n",
    "                error_msg = (\n",
    "                    f\"无法加载数据文件 {base_path}.*\\n\"\n",
    "                    f\"尝试的路径:\\n{chr(10).join(f'- {err}' for err in errors)}\\n\"\n",
    "                    f\"目录 {data_dir} 中的文件:\\n{files_str}\"\n",
    "                )\n",
    "                raise FileNotFoundError(error_msg)\n",
    "            \n",
    "            # 加载数据\n",
    "            athletes_df = try_load_data(\"data/processed/athletes\")\n",
    "            medals_df = try_load_data(\"data/processed/medal_counts\")\n",
    "            \n",
    "            # 数据预处理\n",
    "            athletes_df['Year'] = pd.to_numeric(athletes_df['Year'], errors='coerce')\n",
    "            athletes_df = athletes_df.dropna(subset=['Year', 'NOC', 'Sport'])\n",
    "            \n",
    "            # 转换Medal列为数值\n",
    "            medal_map = {'Gold': 3, 'Silver': 2, 'Bronze': 1}\n",
    "            athletes_df['Medal_Value'] = athletes_df['Medal'].map(medal_map).fillna(0)\n",
    "            \n",
    "            # 数据验证\n",
    "            required_cols = {'athletes': ['Year', 'NOC', 'Sport', 'Medal'],\n",
    "                            'medals': ['Year', 'NOC', 'Gold', 'Total']}\n",
    "                            \n",
    "            for df_name, df in [('athletes', athletes_df), ('medals', medals_df)]:\n",
    "                missing_cols = [col for col in required_cols[df_name] if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    self.console.print(f\"[yellow]警告: {df_name} 数据集的列: {', '.join(df.columns)}[/yellow]\")\n",
    "                    raise ValueError(f\"{df_name} 数据集缺少必要的列: {', '.join(missing_cols)}\")\n",
    "            \n",
    "            return athletes_df, medals_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]数据加载错误: {str(e)}[/bold red]\")\n",
    "            raise\n",
    "\n",
    "    def detect_coach_effect_periods(self, data: pd.DataFrame, country: str, sport: str) -> List[CoachImpact]:\n",
    "        \"\"\"优化后的教练效应检测,增强数值稳定性和错误处理\"\"\"\n",
    "        impacts = []\n",
    "        min_medals = 3\n",
    "        min_gap = 12\n",
    "        window_size = 12\n",
    "\n",
    "        try:\n",
    "            # 输入验证\n",
    "            if data.empty or not isinstance(country, str) or not isinstance(sport, str):\n",
    "                return impacts\n",
    "\n",
    "            # 规范化国家代码\n",
    "            country_map = {'URS': 'RUS', 'GDR': 'GER', 'FRG': 'GER', 'TCH': 'CZE'}\n",
    "            normalized_country = country_map.get(country, country)\n",
    "\n",
    "            # 数据预处理和验证\n",
    "            country_data = data[\n",
    "                (data['NOC'] == normalized_country) &\n",
    "                (data['Sport'] == sport)\n",
    "                ].copy()  # 创建副本避免警告\n",
    "\n",
    "            if country_data.empty:\n",
    "                return impacts\n",
    "\n",
    "            # 年度表现聚合与验证\n",
    "            yearly_perf = country_data.groupby(['Year', 'Team'], observed=True).agg({\n",
    "                'Medal_Value': ['sum', 'count']\n",
    "            }).groupby('Year').agg({\n",
    "                ('Medal_Value', 'sum'): 'sum',\n",
    "                ('Medal_Value', 'count'): 'sum'\n",
    "            }).reset_index()\n",
    "\n",
    "            # 数据验证\n",
    "            if yearly_perf.empty or yearly_perf[('Medal_Value', 'sum')].max() < min_medals:\n",
    "                return impacts\n",
    "\n",
    "            # 异常值处理\n",
    "            medal_values = yearly_perf[('Medal_Value', 'sum')].values\n",
    "            q1, q3 = np.percentile(medal_values, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            outlier_threshold = q3 + 1.5 * iqr\n",
    "            medal_values = np.clip(medal_values, 0, outlier_threshold)\n",
    "            yearly_perf[('Medal_Value', 'sum')] = medal_values\n",
    "\n",
    "            # 窗口分析\n",
    "            for i in range(len(yearly_perf) - window_size + 1):\n",
    "                try:\n",
    "                    window = yearly_perf.iloc[i:i + window_size]\n",
    "                    p1, p2 = window.iloc[:window_size // 2], window.iloc[window_size // 2:]\n",
    "\n",
    "                    # 数值计算保护\n",
    "                    medal_diff = float(p2[('Medal_Value', 'sum')].mean() - p1[('Medal_Value', 'sum')].mean())\n",
    "                    participation_ratio = max(1e-10, p1[('Medal_Value', 'count')].mean())\n",
    "                    participation_change = float(p2[('Medal_Value', 'count')].mean() / participation_ratio - 1)\n",
    "\n",
    "                    period = (int(window['Year'].iloc[0]), int(window['Year'].iloc[-1]))\n",
    "\n",
    "                    # 验证时期重叠\n",
    "                    if abs(medal_diff) >= min_medals and not any(\n",
    "                            abs(period[0] - p[0]) < min_gap or abs(period[1] - p[1]) < min_gap\n",
    "                            for p in [(imp.period[0], imp.period[1]) for imp in impacts]\n",
    "                    ):\n",
    "                        # 统计显著性测试\n",
    "                        p_value = self._bootstrap_significance_test(\n",
    "                            p1[('Medal_Value', 'sum')].values,\n",
    "                            p2[('Medal_Value', 'sum')].values,\n",
    "                            n_bootstrap=3000\n",
    "                        )\n",
    "\n",
    "                        # 一致性评估\n",
    "                        consistency = self._evaluate_performance_consistency(window)\n",
    "\n",
    "                        if p_value < 0.05 and consistency > 0.4:\n",
    "                            impacts.append(CoachImpact(\n",
    "                                sport=sport,\n",
    "                                country=normalized_country,\n",
    "                                period=period,\n",
    "                                medal_change=medal_diff,\n",
    "                                significance=float(1 - p_value),\n",
    "                                consistency=consistency\n",
    "                            ))\n",
    "\n",
    "                except Exception as e:\n",
    "                    self._detect_numerical_issues(f\"Window {i}\", window[('Medal_Value', 'sum')].values)\n",
    "                    continue\n",
    "\n",
    "            return sorted(impacts, key=lambda x: abs(x.medal_change), reverse=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"教练效应检测错误: {str(e)}\")\n",
    "            return impacts\n",
    "    def _calculate_stability_score(self, period1: pd.DataFrame, period2: pd.DataFrame) -> float:\n",
    "        \"\"\"计算表现稳定性得分\"\"\"\n",
    "        std1 = period1['sum'].std()\n",
    "        std2 = period2['sum'].std()\n",
    "        mean1 = period1['sum'].mean()\n",
    "        mean2 = period2['sum'].mean()\n",
    "        \n",
    "        cv1 = std1 / (mean1 + 1)  # 变异系数\n",
    "        cv2 = std2 / (mean2 + 1)\n",
    "        \n",
    "        stability_improvement = (1 / (1 + cv2)) - (1 / (1 + cv1))\n",
    "        return np.clip(stability_improvement, -1, 1)\n",
    "    def _bootstrap_significance_test(self, period1: np.ndarray, period2: np.ndarray, n_bootstrap: int = 3000) -> float:\n",
    "        \"\"\"使用Bootstrap方法进行显著性检验,增加数值稳定性\"\"\"\n",
    "        if len(period1) < 2 or len(period2) < 2:\n",
    "            return 1.0\n",
    "            \n",
    "        try:\n",
    "            # 数据预处理\n",
    "            period1 = np.clip(period1, 0, np.percentile(period1, 99))\n",
    "            period2 = np.clip(period2, 0, np.percentile(period2, 99))\n",
    "            \n",
    "            observed_diff = np.mean(period2) - np.mean(period1)\n",
    "            combined = np.concatenate([period1, period2])\n",
    "            n1, n2 = len(period1), len(period2)\n",
    "            \n",
    "            # Bootstrap重采样\n",
    "            bootstrap_diffs = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                resampled = np.random.choice(combined, size=len(combined), replace=True)\n",
    "                diff = np.mean(resampled[n1:]) - np.mean(resampled[:n1])\n",
    "                bootstrap_diffs.append(diff)\n",
    "            \n",
    "            # 计算p值\n",
    "            p_value = np.mean(np.abs(bootstrap_diffs) >= np.abs(observed_diff))\n",
    "            return float(np.clip(p_value, 1e-8, 1.0))\n",
    "        except:\n",
    "            return 1.0\n",
    "\n",
    "    def _evaluate_performance_consistency(self, data: pd.DataFrame) -> float:\n",
    "        \"\"\"增强版表现一致性评估\"\"\"\n",
    "        try:\n",
    "            # 数据提取和验证\n",
    "            medals = data[('Medal_Value', 'sum')].values.astype(np.float64)\n",
    "            if len(medals) < 3:\n",
    "                return 0.0\n",
    "\n",
    "            # 异常值处理\n",
    "            q1, q3 = np.percentile(medals, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            medals_cleaned = np.clip(medals, q1 - 1.5 * iqr, q3 + 1.5 * iqr)\n",
    "\n",
    "            # 基础统计计算\n",
    "            medals_mean = np.mean(medals_cleaned)\n",
    "            medals_std = np.std(medals_cleaned)\n",
    "\n",
    "            # 数值稳定性检查\n",
    "            if medals_std < 1e-10 or np.isclose(medals_mean, 0, atol=1e-10):\n",
    "                return 0.0\n",
    "\n",
    "            # 标准化处理\n",
    "            medals_scaled = (medals_cleaned - medals_mean) / (medals_std + 1e-10)\n",
    "            years = np.arange(len(medals_scaled), dtype=np.float64)\n",
    "\n",
    "            try:\n",
    "                # 主要方法：SVD分解\n",
    "                X = np.vstack([years, np.ones_like(years)]).T\n",
    "                U, s, Vh = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "                # 条件数检查\n",
    "                if np.max(s) / np.min(s) > 1e10:  # 病态矩阵检查\n",
    "                    raise np.linalg.LinAlgError(\"Matrix is ill-conditioned\")\n",
    "\n",
    "                coef = Vh.T @ np.diag(1 / s) @ U.T @ medals_scaled\n",
    "                trend = coef[0] * years + coef[1]\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                # 备选方法：稳健回归\n",
    "                slope, intercept = stats.theilslopes(medals_scaled, years)[:2]\n",
    "                trend = slope * years + intercept\n",
    "\n",
    "            # 残差分析\n",
    "            residuals = medals_scaled - trend\n",
    "            residuals_std = np.std(residuals)\n",
    "\n",
    "            # 一致性得分计算\n",
    "            base_consistency = 1.0 / (1.0 + residuals_std)\n",
    "\n",
    "            # 趋势权重\n",
    "            trend_weight = np.abs(np.corrcoef(years, medals_scaled)[0, 1])\n",
    "\n",
    "            # 最终一致性得分\n",
    "            final_consistency = 0.7 * base_consistency + 0.3 * trend_weight\n",
    "\n",
    "            return float(np.clip(final_consistency, 0, 1))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"一致性评估错误: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _find_advanced_benchmark(self, data: pd.DataFrame, sport: str, target_country: str) -> Dict:\n",
    "        \"\"\"Improved benchmark country selection with dynamic window and decay\"\"\"\n",
    "\n",
    "        def calculate_sport_strength(country_data: pd.DataFrame, window_years: int = 8) -> float:\n",
    "            \"\"\"Calculate sport strength with temporal weighting\"\"\"\n",
    "            if len(country_data) < 3:\n",
    "                return 0.0\n",
    "\n",
    "            recent_data = country_data.sort_values('Year', ascending=False).head(window_years)\n",
    "            if recent_data.empty:\n",
    "                return 0.0\n",
    "\n",
    "            # Exponential decay weights\n",
    "            years = recent_data['Year'].values\n",
    "            max_year = years.max()\n",
    "            weights = np.exp(-0.2 * (max_year - years))\n",
    "\n",
    "            # Weighted metrics\n",
    "            weighted_medals = np.average(recent_data['Medal_Value'].values, weights=weights)\n",
    "            weighted_trend = np.polyfit(range(len(recent_data)), recent_data['Medal_Value'].values, 1, w=weights)[0]\n",
    "\n",
    "            return 0.7 * weighted_medals + 0.3 * weighted_trend\n",
    "\n",
    "        def calculate_historical_fit(country_data: pd.DataFrame) -> float:\n",
    "            \"\"\"Calculate historical fit score with peak performance consideration\"\"\"\n",
    "            if len(country_data) < 3:\n",
    "                return 0.0\n",
    "\n",
    "            years = country_data['Year'].values\n",
    "            medals = country_data['Medal_Value'].values\n",
    "\n",
    "            # Historical peak\n",
    "            peak_medals = np.percentile(medals, 95)\n",
    "\n",
    "            # Recent performance (last 3 cycles)\n",
    "            recent_mask = years >= (years.max() - 12)\n",
    "            recent_medals = medals[recent_mask] if any(recent_mask) else medals\n",
    "\n",
    "            if len(recent_medals) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            recent_avg = np.mean(recent_medals)\n",
    "\n",
    "            # Consistency factor\n",
    "            consistency = 1 / (1 + np.std(recent_medals))\n",
    "\n",
    "            # Combined score\n",
    "            historical_fit = (0.4 * (recent_avg / peak_medals) +\n",
    "                              0.4 * consistency +\n",
    "                              0.2 * (len(country_data) / 20))  # Experience factor\n",
    "\n",
    "            return float(np.clip(historical_fit, 0, 1))\n",
    "\n",
    "        try:\n",
    "            # Dynamic analysis period based on sport\n",
    "            sport_cycles = {\n",
    "                'Swimming': 8,\n",
    "                'Athletics': 12,\n",
    "                'Gymnastics': 16,\n",
    "                'Volleyball': 8\n",
    "            }\n",
    "            analysis_period = sport_cycles.get(sport, 12)\n",
    "\n",
    "            # Country code normalization\n",
    "            country_mapping = {\n",
    "                'URS': 'RUS', 'GDR': 'GER', 'FRG': 'GER',\n",
    "                'TCH': 'CZE', 'YUG': 'SRB'\n",
    "            }\n",
    "            normalized_country = country_mapping.get(target_country, target_country)\n",
    "\n",
    "            # Get target country data\n",
    "            target_data = data[\n",
    "                (data['Sport'] == sport) &\n",
    "                (data['Year'] >= data['Year'].max() - analysis_period) &\n",
    "                (data['NOC'] == normalized_country)\n",
    "                ]\n",
    "\n",
    "            if target_data.empty:\n",
    "                return None\n",
    "\n",
    "            # Calculate target metrics\n",
    "            target_strength = calculate_sport_strength(target_data)\n",
    "            target_fit = calculate_historical_fit(target_data)\n",
    "\n",
    "            # Calculate current performance with decay\n",
    "            recent_performance = target_data.sort_values('Year', ascending=False)['Medal_Value'].head(3).mean()\n",
    "            if recent_performance == 0:\n",
    "                # Find last medal year and apply decay\n",
    "                last_medal = data[\n",
    "                    (data['Sport'] == sport) &\n",
    "                    (data['NOC'] == normalized_country) &\n",
    "                    (data['Medal_Value'] > 0)\n",
    "                    ].sort_values('Year', ascending=False)\n",
    "\n",
    "                if not last_medal.empty:\n",
    "                    years_since = data['Year'].max() - last_medal['Year'].iloc[0]\n",
    "                    decay_factor = 0.8 ** (years_since / 4)  # 20% decay per cycle\n",
    "                    recent_performance = last_medal['Medal_Value'].iloc[0] * decay_factor\n",
    "\n",
    "            # Find benchmark countries\n",
    "            other_countries = []\n",
    "            for country in data[data['NOC'] != normalized_country]['NOC'].unique():\n",
    "                if country in country_mapping.keys():  # Skip historical countries\n",
    "                    continue\n",
    "\n",
    "                country_data = data[\n",
    "                    (data['Sport'] == sport) &\n",
    "                    (data['Year'] >= data['Year'].max() - analysis_period) &\n",
    "                    (data['NOC'] == country)\n",
    "                    ]\n",
    "\n",
    "                if len(country_data) >= 3:\n",
    "                    strength = calculate_sport_strength(country_data)\n",
    "                    hist_fit = calculate_historical_fit(country_data)\n",
    "\n",
    "                    if strength > target_strength:\n",
    "                        performance = country_data.sort_values('Year', ascending=False)['Medal_Value'].head(3).mean()\n",
    "\n",
    "                        # Monte Carlo simulation for uncertainty\n",
    "                        volatility = {'Swimming': 0.15, 'Gymnastics': 0.25}.get(sport, 0.2)\n",
    "                        simulations = np.random.normal(performance, performance * volatility, 1000)\n",
    "                        ci = np.percentile(simulations, [5, 95])\n",
    "\n",
    "                        other_countries.append({\n",
    "                            'NOC': country,\n",
    "                            'strength': strength,\n",
    "                            'historical_fit': hist_fit,\n",
    "                            'performance': performance,\n",
    "                            'uncertainty': (ci[0], ci[1]),\n",
    "                            'score': 0.4 * strength + 0.3 * hist_fit + 0.3 * (performance / (target_strength + 1e-6))\n",
    "                        })\n",
    "\n",
    "            if not other_countries:\n",
    "                return None\n",
    "\n",
    "            # Select best benchmark\n",
    "            best_benchmark = max(other_countries, key=lambda x: x['score'])\n",
    "\n",
    "            # Calculate normalized improvement potential\n",
    "            sport_max = data.groupby('Sport')['Medal_Value'].max()[sport]\n",
    "            improvement_potential = (best_benchmark['performance'] - recent_performance) / (sport_max + 1e-6)\n",
    "\n",
    "            return {\n",
    "                'sport': sport,\n",
    "                'benchmark_country': str(best_benchmark['NOC']),\n",
    "                'current_performance': float(recent_performance),\n",
    "                'improvement_potential': float(np.clip(improvement_potential, 0, 1)),\n",
    "                'historical_fit': float(target_fit),\n",
    "                'estimated_medal_gain': {\n",
    "                    'mean': float(best_benchmark['performance'] - recent_performance),\n",
    "                    'range': f\"{best_benchmark['uncertainty'][0]:.1f}-{best_benchmark['uncertainty'][1]:.1f}\"\n",
    "                },\n",
    "                'benchmark_metrics': {\n",
    "                    'avg_performance': float(best_benchmark['performance']),\n",
    "                    'stability': float(best_benchmark['historical_fit']),\n",
    "                    'experience_years': int(len(target_data))\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Benchmark analysis error: {str(e)}\")\n",
    "            return None\n",
    "    def _calculate_country_metrics(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"优化后的国家表现指标计算\"\"\"\n",
    "        if data.empty:\n",
    "            return {\n",
    "                'avg_performance': 0.0,\n",
    "                'recent_performance': 0.0,\n",
    "                'stability': 0.0,\n",
    "                'trend_score': 0.0,\n",
    "                'consistency': 0.0,\n",
    "                'experience': 0\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            years = data['Year'].values\n",
    "            medals = data['Medal_Value'].values\n",
    "            \n",
    "            # 基础统计\n",
    "            recent_perf = medals[-4:].mean() if len(medals) >= 4 else medals.mean()\n",
    "            avg_perf = medals.mean()\n",
    "            stability = 1 / (1 + np.std(medals))\n",
    "            \n",
    "            # 趋势分析，增加错误处理\n",
    "            try:\n",
    "                if len(years) > 2:\n",
    "                    X = (years - years.min()).reshape(-1, 1)\n",
    "                    y = medals.reshape(-1, 1)\n",
    "                    slope = np.polyfit(X.ravel(), y.ravel(), 1, rcond=1e-10)[0]\n",
    "                    trend_score = np.clip(slope / (avg_perf + 1), -1, 1)\n",
    "                else:\n",
    "                    trend_score = 0.0\n",
    "            except:\n",
    "                trend_score = 0.0\n",
    "            \n",
    "            # 一致性评估\n",
    "            consistency = 1 - np.std(medals) / (avg_perf + 1)\n",
    "            \n",
    "            return {\n",
    "                'avg_performance': avg_perf,\n",
    "                'recent_performance': recent_perf,\n",
    "                'stability': stability,\n",
    "                'trend_score': float(trend_score),\n",
    "                'consistency': consistency,\n",
    "                'experience': len(years)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'avg_performance': 0.0,\n",
    "                'recent_performance': 0.0,\n",
    "                'stability': 0.0,\n",
    "                'trend_score': 0.0,\n",
    "                'consistency': 0.0,\n",
    "                'experience': 0\n",
    "            }\n",
    "\n",
    "    def analyze_great_coach_effect(self,\n",
    "                                 athletes_df: pd.DataFrame,\n",
    "                                 medals_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"分析\"伟大教练\"效应\"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # 选择重点分析的运动项目\n",
    "        focus_sports = ['Gymnastics', 'Swimming', 'Athletics', 'Volleyball']\n",
    "\n",
    "        # 分析每个重点项目\n",
    "        for sport in focus_sports:\n",
    "            sport_results = {}\n",
    "\n",
    "            # 获取该项目的主要参赛国\n",
    "            top_countries = athletes_df[\n",
    "                athletes_df['Sport'] == sport\n",
    "            ]['NOC'].value_counts().head(10).index\n",
    "\n",
    "            for country in top_countries:\n",
    "                impacts = self.detect_coach_effect_periods(athletes_df, country, sport)\n",
    "                if impacts:\n",
    "                    sport_results[country] = impacts\n",
    "\n",
    "            results[sport] = sport_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def recommend_coach_investments(self, athletes_df: pd.DataFrame, medals_df: pd.DataFrame,\n",
    "                                    countries: List[str]) -> Dict:\n",
    "        \"\"\"增强版教练投资建议系统\"\"\"\n",
    "\n",
    "        def analyze_historical_data(data: pd.DataFrame, country: str, sport: str) -> Dict:\n",
    "            \"\"\"分析历史数据并计算关键指标\"\"\"\n",
    "            print(f\"\\n分析 {country} 在 {sport} 项目的历史数据:\")\n",
    "\n",
    "            # 检查原始数据\n",
    "            print(f\"数据集中unique的Sport值: {data['Sport'].unique()}\")\n",
    "            print(f\"数据集中unique的NOC值: {data['NOC'].unique()}\")\n",
    "\n",
    "            # 放宽匹配条件，使用模糊匹配\n",
    "            sport_mask = data['Sport'].str.contains(sport, case=False, na=False)\n",
    "            country_mask = data['NOC'].str.contains(country, case=False, na=False)\n",
    "\n",
    "            country_data = data[sport_mask & country_mask].copy()\n",
    "\n",
    "            print(f\"找到 {len(country_data)} 条原始数据记录\")\n",
    "            if len(country_data) == 0:\n",
    "                print(f\"尝试更广泛的搜索...\")\n",
    "                # 尝试查找可能的运动项目名称变体\n",
    "                possible_sports = data[data['Sport'].str.contains(sport[:4], case=False, na=False)]['Sport'].unique()\n",
    "                print(f\"可能的运动项目: {possible_sports}\")\n",
    "\n",
    "                # 尝试查找可能的国家代码变体\n",
    "                possible_countries = data[data['NOC'].str.contains(country[:2], case=False, na=False)]['NOC'].unique()\n",
    "                print(f\"可能的国家代码: {possible_countries}\")\n",
    "\n",
    "                # 使用更宽松的匹配\n",
    "                sport_mask = data['Sport'].isin(possible_sports)\n",
    "                country_mask = data['NOC'].isin(possible_countries)\n",
    "                country_data = data[sport_mask & country_mask].copy()\n",
    "                print(f\"宽松匹配后找到 {len(country_data)} 条记录\")\n",
    "\n",
    "            if len(country_data) == 0:\n",
    "                print(f\"警告: {country} 在 {sport} 项目中没有数据\")\n",
    "                return create_empty_metrics()\n",
    "\n",
    "            # 验证数据的完整性\n",
    "            print(\"\\n数据验证:\")\n",
    "            print(f\"年份范围: {country_data['Year'].min()} - {country_data['Year'].max()}\")\n",
    "            print(f\"Medal_Value统计: \\n{country_data['Medal_Value'].describe()}\")\n",
    "\n",
    "            # 计算年度统计\n",
    "            yearly_medals = country_data.groupby('Year')['Medal_Value'].sum()\n",
    "            print(f\"\\n年度奖牌统计:\\n{yearly_medals}\")\n",
    "\n",
    "            # 处理异常值\n",
    "            q1, q3 = yearly_medals.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            yearly_medals = yearly_medals.clip(upper=upper_bound)\n",
    "\n",
    "            # 计算近期表现 (最近8年)\n",
    "            recent_years = yearly_medals.index >= yearly_medals.index.max() - 8\n",
    "            recent_performance = yearly_medals[recent_years].mean() if any(recent_years) else 0\n",
    "            print(f\"近期平均表现: {recent_performance}\")\n",
    "\n",
    "            # 计算趋势\n",
    "            if len(yearly_medals) >= 2:\n",
    "                years = np.array(range(len(yearly_medals)))\n",
    "                trend = np.polyfit(years, yearly_medals.values, 1)[0]\n",
    "            else:\n",
    "                trend = 0\n",
    "            print(f\"趋势系数: {trend}\")\n",
    "\n",
    "            # 计算历史适配度指标\n",
    "            total_medals = yearly_medals.sum()\n",
    "            n_years = len(yearly_medals)\n",
    "            span_years = yearly_medals.index.max() - yearly_medals.index.min() + 4\n",
    "            participation_rate = n_years * 4 / span_years if span_years > 0 else 0\n",
    "\n",
    "            # 计算稳定性\n",
    "            consistency = 1 / (1 + yearly_medals.std()) if len(yearly_medals) > 1 else 0\n",
    "\n",
    "            print(f\"\"\"\n",
    "        详细统计:\n",
    "        - 总奖牌数: {total_medals}\n",
    "        - 参赛年数: {n_years}\n",
    "        - 参与率: {participation_rate:.2f}\n",
    "        - 稳定性: {consistency:.2f}\n",
    "        \"\"\")\n",
    "\n",
    "            # 综合历史适配度\n",
    "            historical_fit = (\n",
    "                    0.4 * (total_medals / (n_years + 1)) / 10 +  # 归一化平均奖牌数\n",
    "                    0.3 * participation_rate +\n",
    "                    0.3 * consistency\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                'peak': float(yearly_medals.max()),\n",
    "                'recent': float(recent_performance),\n",
    "                'trend': float(trend),\n",
    "                'years': int(n_years),\n",
    "                'historical_fit': float(historical_fit),\n",
    "                'consistency': float(consistency),\n",
    "                'total_medals': int(total_medals)\n",
    "            }\n",
    "\n",
    "            print(f\"最终指标:\\n{metrics}\")\n",
    "            return metrics\n",
    "\n",
    "        def create_empty_metrics():\n",
    "            \"\"\"创建空指标\"\"\"\n",
    "            return {\n",
    "                'peak': 0,\n",
    "                'recent': 0,\n",
    "                'trend': 0,\n",
    "                'years': 0,\n",
    "                'historical_fit': 0,\n",
    "                'consistency': 0,\n",
    "                'total_medals': 0\n",
    "            }\n",
    "\n",
    "        def evaluate_improvement_potential(target: Dict, benchmark: Dict) -> float:\n",
    "            \"\"\"评估改进潜力\"\"\"\n",
    "            print(f\"\\n评估改进潜力:\")\n",
    "            print(f\"目标指标: {target}\")\n",
    "            print(f\"标杆指标: {benchmark}\")\n",
    "\n",
    "            # 如果是新项目或历史表现较弱\n",
    "            if target['total_medals'] < 3:\n",
    "                base_potential = 0.3\n",
    "                print(f\"新项目或历史较弱，基础潜力: {base_potential}\")\n",
    "                return base_potential\n",
    "\n",
    "            # 计算相对差距\n",
    "            relative_gap = (benchmark['recent'] - target['recent']) / (benchmark['recent'] + 1)\n",
    "            print(f\"相对差距: {relative_gap}\")\n",
    "\n",
    "            # 趋势因子\n",
    "            trend_factor = np.tanh(max(0, benchmark['trend'] - target['trend']))\n",
    "            print(f\"趋势因子: {trend_factor}\")\n",
    "\n",
    "            # 历史因子\n",
    "            historical_factor = benchmark['historical_fit'] / (target['historical_fit'] + 0.1)\n",
    "            print(f\"历史因子: {historical_factor}\")\n",
    "\n",
    "            # 综合评分\n",
    "            potential = (\n",
    "                    0.5 * relative_gap +\n",
    "                    0.3 * trend_factor +\n",
    "                    0.2 * min(historical_factor, 2)  # 限制历史因子的影响\n",
    "            )\n",
    "\n",
    "            final_potential = float(np.clip(potential, 0, 1))\n",
    "            print(f\"最终潜力评分: {final_potential}\")\n",
    "            return final_potential\n",
    "\n",
    "        try:\n",
    "            recommendations = {}\n",
    "            print(f\"\\n开始生成教练投资建议...\")\n",
    "\n",
    "            # 基础运动项目\n",
    "            sports = ['Swimming', 'Athletics', 'Gymnastics', 'Volleyball']\n",
    "\n",
    "            # 合理的最大奖牌预期\n",
    "            max_medals = {\n",
    "                'Swimming': 15,\n",
    "                'Athletics': 12,\n",
    "                'Gymnastics': 10,\n",
    "                'Volleyball': 8\n",
    "            }\n",
    "\n",
    "            for country in countries:\n",
    "                print(f\"\\n分析 {country} 的投资机会:\")\n",
    "                country_recommendations = []\n",
    "\n",
    "                for sport in sports:\n",
    "                    print(f\"\\n评估 {sport} 项目:\")\n",
    "                    try:\n",
    "                        # 获取目标国家数据\n",
    "                        target_metrics = analyze_historical_data(athletes_df, country, sport)\n",
    "\n",
    "                        # 获取标杆国家数据\n",
    "                        benchmark_data = athletes_df[\n",
    "                            (athletes_df['Sport'] == sport) &\n",
    "                            (athletes_df['NOC'] != country) &\n",
    "                            (~athletes_df['NOC'].isin(['URS', 'GDR', 'FRG', 'TCH', 'YUG']))\n",
    "                            ]\n",
    "\n",
    "                        # 选择最佳表现的国家作为标杆\n",
    "                        benchmark_country = benchmark_data.groupby('NOC')['Medal_Value'].sum().nlargest(1).index[0]\n",
    "                        print(f\"选择的标杆国家: {benchmark_country}\")\n",
    "\n",
    "                        benchmark_metrics = analyze_historical_data(athletes_df, benchmark_country, sport)\n",
    "\n",
    "                        # 计算潜力\n",
    "                        potential = evaluate_improvement_potential(target_metrics, benchmark_metrics)\n",
    "\n",
    "                        # 计算预期奖牌增长\n",
    "                        medal_gain = min(\n",
    "                            max_medals[sport],\n",
    "                            benchmark_metrics['recent'] - target_metrics['recent']\n",
    "                        )\n",
    "\n",
    "                        if potential >= 0.1:  # 降低潜力阈值\n",
    "                            country_recommendations.append({\n",
    "                                'sport': sport,\n",
    "                                'benchmark_country': benchmark_country,\n",
    "                                'current_performance': float(target_metrics['recent']),\n",
    "                                'improvement_potential': float(potential),\n",
    "                                'historical_fit': float(target_metrics['historical_fit']),\n",
    "                                'estimated_medal_gain': {\n",
    "                                    'mean': float(medal_gain),\n",
    "                                    'range': f\"{(medal_gain * 0.85):.1f}-{(medal_gain * 1.15):.1f}\"\n",
    "                                },\n",
    "                                'benchmark_metrics': {\n",
    "                                    'avg_performance': float(benchmark_metrics['recent']),\n",
    "                                    'trend': float(benchmark_metrics['trend']),\n",
    "                                    'experience_years': int(benchmark_metrics['years'])\n",
    "                                }\n",
    "                            })\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理 {sport} 时出错: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                recommendations[country] = sorted(\n",
    "                    country_recommendations,\n",
    "                    key=lambda x: x['improvement_potential'],\n",
    "                    reverse=True\n",
    "                )\n",
    "\n",
    "            return recommendations\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"推荐系统错误: {str(e)}\")\n",
    "            return {}\n",
    "    def _find_benchmark_country(self, \n",
    "                              data: pd.DataFrame, \n",
    "                              sport: str, \n",
    "                              exclude_country: str) -> str:\n",
    "        \"\"\"找到某个运动项目的标杆国家\"\"\"\n",
    "        performance = data[\n",
    "            (data['Sport'] == sport) & \n",
    "            (data['NOC'] != exclude_country)\n",
    "        ].groupby('NOC')['Medal_Value'].mean()\n",
    "        \n",
    "        return performance.nlargest(1).index[0] if not performance.empty else None\n",
    "\n",
    "    def _calculate_improvement_potential(self, \n",
    "                                      data: pd.DataFrame, \n",
    "                                      sport: str, \n",
    "                                      country: str, \n",
    "                                      benchmark_country: str) -> float:\n",
    "        \"\"\"计算潜在提升空间\"\"\"\n",
    "        country_perf = data[\n",
    "            (data['Sport'] == sport) & \n",
    "            (data['NOC'] == country)\n",
    "        ]['Medal_Value'].mean()\n",
    "        \n",
    "        benchmark_perf = data[\n",
    "            (data['Sport'] == sport) & \n",
    "            (data['NOC'] == benchmark_country)\n",
    "        ]['Medal_Value'].mean()\n",
    "        \n",
    "        return max(0, benchmark_perf - country_perf)\n",
    "\n",
    "    def generate_report(self, coach_effects: Dict, recommendations: Dict) -> str:\n",
    "        \"\"\"优化的报告生成器\"\"\"\n",
    "        report_lines = []\n",
    "\n",
    "        # 1. 教练效应分析\n",
    "        report_lines.extend([\n",
    "            \"1. '伟大教练'效应分析\",\n",
    "            \"-\" * 50,\n",
    "            \"\"\n",
    "        ])\n",
    "\n",
    "        for sport, countries_data in coach_effects.items():\n",
    "            if countries_data:  # Only add sport section if there's data\n",
    "                report_lines.append(f\"\\n{sport}:\")\n",
    "                for country, impacts in countries_data.items():\n",
    "                    for impact in impacts:\n",
    "                        report_lines.append(\n",
    "                            f\"  - {country} ({impact.period[0]}-{impact.period[1]}): \"\n",
    "                            f\"奖牌变化 {impact.medal_change:.1f}, \"\n",
    "                            f\"显著性 {impact.significance:.2f}, \"\n",
    "                            f\"一致性 {impact.consistency:.2f}\"\n",
    "                        )\n",
    "\n",
    "        # 2. 教练投资建议\n",
    "        report_lines.extend([\n",
    "            \"\\n\\n2. 教练投资建议\",\n",
    "            \"-\" * 50,\n",
    "            \"\"\n",
    "        ])\n",
    "\n",
    "        for country, recommendations_list in recommendations.items():\n",
    "            report_lines.append(f\"\\n{country}的优先投资项目:\")\n",
    "            if recommendations_list:  # Check if there are recommendations\n",
    "                for rec in recommendations_list:\n",
    "                    report_lines.extend([\n",
    "                        f\"  - {rec['sport']}:\",\n",
    "                        f\"    * 标杆国家: {rec['benchmark_country']}\",\n",
    "                        f\"    * 当前水平: {rec['current_performance']:.2f}\",\n",
    "                        f\"    * 提升潜力: {rec['improvement_potential']:.2f}\",\n",
    "                        f\"    * 历史适配度: {rec['historical_fit']:.2f}\",\n",
    "                        f\"    * 预期奖牌增长: {rec['estimated_medal_gain']['mean']:.2f} ({rec['estimated_medal_gain']['range']})\",\n",
    "                        f\"    * 标杆国绩效:\",\n",
    "                        f\"      - 平均表现: {rec['benchmark_metrics']['avg_performance']:.2f}\",\n",
    "                        f\"      - 发展趋势: {rec['benchmark_metrics']['trend']:.2f}\",\n",
    "                        f\"      - 发展年限: {rec['benchmark_metrics']['experience_years']}\"\n",
    "                    ])\n",
    "            else:\n",
    "                report_lines.append(\"  暂无优先投资建议\")\n",
    "\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "    def _detect_numerical_issues(self, label: str, matrix: np.ndarray):\n",
    "        \"\"\"数值问题诊断\"\"\"\n",
    "        issues = []\n",
    "        if np.any(np.isnan(matrix)):\n",
    "            issues.append(f\"{label} 包含NaN\")\n",
    "        if np.any(np.isinf(matrix)):\n",
    "            issues.append(f\"{label} 包含Inf\")\n",
    "        if np.any(np.abs(matrix) < 1e-10):\n",
    "            issues.append(f\"{label} 包含接近零值\")\n",
    "        if len(issues) > 0:\n",
    "            print(f\"数值问题 - {', '.join(issues)}\")\n",
    "            print(f\"矩阵统计: 形状{matrix.shape}, 最小值{np.min(matrix)}, 最大值{np.max(matrix)}\")\n",
    "def main():\n",
    "    console = Console()\n",
    "    \n",
    "    try:\n",
    "        # 初始化分析器\n",
    "        analyzer = CoachImpactAnalyzer()\n",
    "        \n",
    "        # 加载数据\n",
    "        console.print(\"[bold cyan]加载数据...[/bold cyan]\")\n",
    "        athletes_df, medals_df = analyzer.load_and_prepare_data()\n",
    "        \n",
    "        # 分析教练效应\n",
    "        console.print(\"[bold cyan]分析'伟大教练'效应...[/bold cyan]\")\n",
    "        coach_effects = analyzer.analyze_great_coach_effect(athletes_df, medals_df)\n",
    "        \n",
    "        # 为特定国家生成建议\n",
    "        target_countries = ['France', 'Germany', 'Italy']  # 示例国家\n",
    "        console.print(\"[bold cyan]生成教练投资建议...[/bold cyan]\")\n",
    "        recommendations = analyzer.recommend_coach_investments(\n",
    "            athletes_df, medals_df, target_countries\n",
    "        )\n",
    "        \n",
    "        # 生成报告\n",
    "        report = analyzer.generate_report(coach_effects, recommendations)\n",
    "        \n",
    "        # 保存报告\n",
    "        output_dir = Path(\"analysis_results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_dir / \"coach_impact_analysis_report.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        # 显示报告\n",
    "        console.print(\"\\n[bold green]分析报告:[/bold green]\")\n",
    "        console.print(report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]错误: {str(e)}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用教程https://www.yuque.com/appoint-igkhv/ex5pwc/wy14bsvlfza64iqc\n",
    "\n",
    "遇到任何使用问题都可以联系我们，24小时人工客服在线，所有信息我们都会回复，请不要随意退款，没问题请给我们打个五星好评，对客服评价也打个满意，下次续费有优惠，因为为虚拟物品，如果对商品不满意或者问题无法解决，退款时候退款售后类型请不要选退货退款，选择第一个退款即可（不想要了）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24ecd4d6-ec8d-47fe-884f-33d0e3bacb52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn import clone\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from typing import Dict, List, Tuple\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from rich.progress import Progress\n",
    "class OlympicMedalPredictor:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.models = {\n",
    "            'gbm': GradientBoostingRegressor(\n",
    "                n_estimators=200, \n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=6, \n",
    "                random_state=42\n",
    "            ),\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                min_child_samples=20,\n",
    "                min_child_weight=1e-3,\n",
    "                reg_lambda=1.0,\n",
    "                reg_alpha=0.0,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        self.model_weights = {}\n",
    "        self.feature_importance = {}\n",
    "        self.predictions_store = {}\n",
    "        \n",
    "    def prepare_data(self, features_df: pd.DataFrame, historical_data: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, pd.Series], List[str]]:\n",
    "        \"\"\"强化的数据准备函数，处理无穷值和异常值\"\"\"\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        import numpy as np\n",
    "        \n",
    "        df = features_df.merge(\n",
    "            historical_data[['NOC', 'Year', 'Gold', 'Total']], \n",
    "            on=['NOC', 'Year'], \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # 处理历史趋势特征\n",
    "        for col in ['Gold', 'Total']:\n",
    "            # 移动平均，使用min_periods避免NaN\n",
    "            df[f'{col}_ma_4year'] = df.groupby('NOC')[col].transform(\n",
    "                lambda x: x.rolling(4, min_periods=1).mean()\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # 安全的增长率计算\n",
    "            def safe_pct_change(x):\n",
    "                change = x.pct_change(4)\n",
    "                # 将inf替换为上限值\n",
    "                change = change.replace([np.inf, -np.inf], np.nan)\n",
    "                # 使用90分位数作为极值上限\n",
    "                upper_bound = np.nanpercentile(change, 90)\n",
    "                return change.clip(lower=-1, upper=upper_bound)\n",
    "                \n",
    "            df[f'{col}_growth'] = df.groupby('NOC')[col].transform(safe_pct_change).fillna(0)\n",
    "            df[f'historical_max_{col}'] = df.groupby('NOC')[col].transform('max').fillna(0)\n",
    "        \n",
    "        # 分层标签\n",
    "        tier_1_countries = ['United States', 'China', 'Great Britain', 'Japan', 'Australia']\n",
    "        tier_2_countries = ['France', 'Germany', 'Italy', 'Netherlands', 'South Korea']\n",
    "        df['country_tier'] = (\n",
    "            df['NOC'].apply(\n",
    "                lambda x: 1 if x in tier_1_countries else (2 if x in tier_2_countries else 3)\n",
    "            )\n",
    "            .astype(int)  # 强制转换为整数类型\n",
    "        )\n",
    "        \n",
    "        # 保存NOC列\n",
    "        noc_series = df['NOC']\n",
    "        \n",
    "        # 分离特征和目标变量\n",
    "        target_cols = ['Gold', 'Total']\n",
    "        exclude_cols = ['NOC'] + target_cols\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # 提取特征矩阵\n",
    "        X = df[feature_cols].copy()\n",
    "        \n",
    "        # 处理数值型特征\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if len(num_cols) > 0:\n",
    "            # 先处理无穷值\n",
    "            X[num_cols] = X[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "            # 使用中位数填充缺失值\n",
    "            num_imputer = SimpleImputer(strategy='median')\n",
    "            X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "        \n",
    "        # 类别型特征处理\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(cat_cols) > 0:\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "        \n",
    "        # 最终数据类型转换和清理\n",
    "        X = X.astype(float)\n",
    "        \n",
    "        # 确保没有异常值\n",
    "        for col in X.columns:\n",
    "            upper_bound = np.percentile(X[col], 99)\n",
    "            lower_bound = np.percentile(X[col], 1)\n",
    "            X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "        \n",
    "        y = {\n",
    "            'Gold': df['Gold'].fillna(0).astype(float),\n",
    "            'Total': df['Total'].fillna(0).astype(float)\n",
    "        }\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "    \n",
    "\n",
    "\n",
    "    def _optimize_ensemble_weights(self, \n",
    "                                predictions: Dict[str, np.ndarray], \n",
    "                                target: pd.Series,\n",
    "                                base_scores: Dict[str, float],\n",
    "                                constraints: Dict) -> Dict[str, float]:\n",
    "        \"\"\"优化集成权重，考虑历史约束\"\"\"\n",
    "        def objective(w):\n",
    "            w = w / w.sum()\n",
    "            ensemble_pred = np.zeros_like(list(predictions.values())[0])\n",
    "            for i, (_, pred) in enumerate(predictions.items()):\n",
    "                ensemble_pred += w[i] * pred\n",
    "            \n",
    "            # 添加约束惩罚\n",
    "            penalty = 0\n",
    "            if ensemble_pred.sum() < constraints['total_range'][0]:\n",
    "                penalty += (constraints['total_range'][0] - ensemble_pred.sum()) ** 2\n",
    "            elif ensemble_pred.sum() > constraints['total_range'][1]:\n",
    "                penalty += (ensemble_pred.sum() - constraints['total_range'][1]) ** 2\n",
    "            \n",
    "            return -r2_score(target, ensemble_pred) + penalty * 0.1\n",
    "        \n",
    "        n_models = len(predictions)\n",
    "        weights = np.array([base_scores[model] for model in predictions.keys()])\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        constraints_opt = (\n",
    "            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "        )\n",
    "        bounds = [(0, 1) for _ in range(n_models)]\n",
    "        \n",
    "        result = minimize(\n",
    "            objective, \n",
    "            weights, \n",
    "            method='SLSQP',\n",
    "            constraints=constraints_opt,\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': 1000}\n",
    "        )\n",
    "        \n",
    "        optimized_weights = result.x / result.x.sum()\n",
    "        return dict(zip(predictions.keys(), optimized_weights))\n",
    "    \n",
    "    def _calculate_weights(self, scores: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"计算模型权重\"\"\"\n",
    "        total = sum(scores.values())\n",
    "        weights = {model: score/total for model, score in scores.items()}\n",
    "        return weights\n",
    "    \n",
    "    def train_models(self, X: pd.DataFrame, y: Dict[str, pd.Series]) -> Dict:\n",
    "        trained_models = {}\n",
    "        scores = {}\n",
    "        \n",
    "        historical_constraints = {\n",
    "            'Gold': {\n",
    "                'top_countries': {\n",
    "                    'China': (35, 45), 'United States': (30, 40),\n",
    "                    'Great Britain': (20, 30), 'ROC': (20, 30), \n",
    "                    'Japan': (20, 30)\n",
    "                },\n",
    "                'min': 0, 'max': 45, 'total_range': (300, 340)\n",
    "            },\n",
    "            'Total': {\n",
    "                'top_countries': {\n",
    "                    'China': (80, 100), 'United States': (80, 100),\n",
    "                    'Great Britain': (50, 70), 'ROC': (50, 70),\n",
    "                    'Japan': (50, 70)\n",
    "                },\n",
    "                'min': 0, 'max': 100, 'total_range': (950, 1100)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with Progress() as progress:\n",
    "            total_tasks = len(y) * len(self.models)\n",
    "            train_progress = progress.add_task(\"[cyan]训练模型...\", total=total_tasks)\n",
    "            \n",
    "            for target_name, target in y.items():\n",
    "                if target is None:\n",
    "                    continue\n",
    "                    \n",
    "                trained_models[target_name] = {}\n",
    "                scores[target_name] = {}\n",
    "                self.predictions_store[target_name] = {}\n",
    "                self.model_weights[target_name] = {}\n",
    "                \n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                \n",
    "                for model_name, model in self.models.items():\n",
    "                    model_scores = []\n",
    "                    fold_predictions = []\n",
    "                    fold_actuals = []\n",
    "                    \n",
    "                    # 模型参数设置\n",
    "                    if model_name == 'xgb':\n",
    "                        model.set_params(learning_rate=0.05, n_estimators=300)\n",
    "                    elif model_name == 'lgb':\n",
    "                        model.set_params(learning_rate=0.05, n_estimators=300) \n",
    "                    elif model_name == 'gbm':\n",
    "                        model.set_params(learning_rate=0.05, n_estimators=300)\n",
    "                    elif model_name == 'rf':\n",
    "                        model.set_params(n_estimators=300)\n",
    "                    \n",
    "                    for train_idx, val_idx in tscv.split(X):\n",
    "                        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                        y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n",
    "                        \n",
    "                        # 模型训练\n",
    "                        if model_name in ['xgb', 'lgb']:\n",
    "                            model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "                        else:\n",
    "                            model.fit(X_train, y_train)\n",
    "                        \n",
    "                        pred = model.predict(X_val)\n",
    "                        pred = np.clip(\n",
    "                            pred,\n",
    "                            historical_constraints[target_name]['min'],\n",
    "                            historical_constraints[target_name]['max']\n",
    "                        )\n",
    "                        \n",
    "                        fold_predictions.extend(pred)\n",
    "                        fold_actuals.extend(y_val)\n",
    "                        score = r2_score(y_val, pred)\n",
    "                        model_scores.append(score)\n",
    "                    \n",
    "                    trained_models[target_name][model_name] = model\n",
    "                    scores[target_name][model_name] = np.mean(model_scores)\n",
    "                    \n",
    "                    if hasattr(model, 'feature_importances_'):\n",
    "                        self.feature_importance[f\"{target_name}_{model_name}\"] = pd.Series(\n",
    "                            model.feature_importances_,\n",
    "                            index=X.columns\n",
    "                        ).sort_values(ascending=False)\n",
    "                    \n",
    "                    progress.update(train_progress, advance=1)\n",
    "                \n",
    "                weights = self._optimize_ensemble_weights(\n",
    "                    {name: model.predict(X) for name, model in trained_models[target_name].items()},\n",
    "                    target,\n",
    "                    scores[target_name],\n",
    "                    historical_constraints[target_name]\n",
    "                )\n",
    "                self.model_weights[target_name].update(weights)\n",
    "        \n",
    "        return trained_models\n",
    "\n",
    "    def _calculate_optimal_weights(self, predictions: Dict, scores: Dict) -> Dict[str, float]:\n",
    "        \"\"\"计算最优权重\"\"\"\n",
    "        base_weights = np.array([scores[model] for model in predictions.keys()])\n",
    "        weights = base_weights / np.sum(base_weights)\n",
    "        \n",
    "        # 验证预测结果的表现\n",
    "        ensemble_predictions = np.zeros_like(list(predictions.values())[0]['predictions'])\n",
    "        for i, (model_name, _) in enumerate(predictions.items()):\n",
    "            ensemble_predictions += weights[i] * predictions[model_name]['predictions']\n",
    "        \n",
    "        # 返回归一化的权重\n",
    "        return dict(zip(predictions.keys(), weights))\n",
    "\n",
    "    def predict_with_uncertainty(self, X_pred: pd.DataFrame, models: Dict, target_name: str, n_iterations: int = 500) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"改进的预测函数，修复了权重应用和约束问题\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # 更细致的历史基线设置\n",
    "        historical_baselines = {\n",
    "            'Gold': {\n",
    "                'tier_1': {\n",
    "                    'min': 25, 'max': 40,  # 适当降低上限\n",
    "                    'noise': 0.03,  # 降低噪声\n",
    "                    'countries': ['United States', 'China']\n",
    "                },\n",
    "                'tier_2': {\n",
    "                    'min': 15, 'max': 25,\n",
    "                    'noise': 0.05,\n",
    "                    'countries': ['Great Britain', 'Japan', 'Australia']\n",
    "                },\n",
    "                'tier_3': {\n",
    "                    'min': 8, 'max': 15,\n",
    "                    'noise': 0.08,\n",
    "                    'countries': ['France', 'Germany', 'Italy', 'Netherlands']\n",
    "                },\n",
    "                'tier_4': {\n",
    "                    'min': 0, 'max': 8,\n",
    "                    'noise': 0.10\n",
    "                },\n",
    "                'total_range': (300, 340)\n",
    "            },\n",
    "            'Total': {\n",
    "                'tier_1': {\n",
    "                    'min': 70, 'max': 110,\n",
    "                    'noise': 0.03,\n",
    "                    'countries': ['United States', 'China']\n",
    "                },\n",
    "                'tier_2': {\n",
    "                    'min': 50, 'max': 70,\n",
    "                    'noise': 0.05,\n",
    "                    'countries': ['Great Britain', 'Japan', 'Australia']\n",
    "                },\n",
    "                'tier_3': {\n",
    "                    'min': 30, 'max': 50,\n",
    "                    'noise': 0.08,\n",
    "                    'countries': ['France', 'Germany', 'Italy', 'Netherlands']\n",
    "                },\n",
    "                'tier_4': {\n",
    "                    'min': 0, 'max': 30,\n",
    "                    'noise': 0.10\n",
    "                },\n",
    "                'total_range': (950, 1100)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\"[cyan]生成预测...\", total=n_iterations)\n",
    "            \n",
    "            for _ in range(n_iterations):\n",
    "                model_preds = {}\n",
    "                \n",
    "                # 为每个模型生成预测\n",
    "                for model_name, model in models[target_name].items():\n",
    "                    X_noisy = X_pred.copy()\n",
    "                    \n",
    "                    # 基于国家分层应用不同的噪声和约束\n",
    "                    predictions_array = np.zeros(len(X_pred))\n",
    "                    \n",
    "                    for country_idx, country in enumerate(X_pred.index):\n",
    "                        # 确定国家所属层级\n",
    "                        tier = None\n",
    "                        for tier_name in ['tier_1', 'tier_2', 'tier_3']:\n",
    "                            if country in historical_baselines[target_name][tier_name]['countries']:\n",
    "                                tier = tier_name\n",
    "                                break\n",
    "                        if tier is None:\n",
    "                            tier = 'tier_4'\n",
    "                        \n",
    "                        # 获取该层级的参数\n",
    "                        tier_params = historical_baselines[target_name][tier]\n",
    "                        \n",
    "                        # 生成带噪声的预测\n",
    "                        noise = np.random.normal(0, tier_params['noise'])\n",
    "                        X_noisy_row = X_noisy.iloc[[country_idx]] * (1 + noise)\n",
    "                        pred = model.predict(X_noisy_row)[0]\n",
    "                        \n",
    "                        # 应用层级特定的约束\n",
    "                        pred = np.clip(pred, tier_params['min'], tier_params['max'])\n",
    "                        \n",
    "                        # 东道主效应 (美国)\n",
    "                        if country == 'United States':\n",
    "                            pred *= 1.05  # 降低东道主加成\n",
    "                        \n",
    "                        predictions_array[country_idx] = pred\n",
    "                    \n",
    "                    model_preds[model_name] = predictions_array\n",
    "                \n",
    "                # 使用模型权重进行集成\n",
    "                weights = np.array([self.model_weights[target_name].get(name, 1.0/len(model_preds)) \n",
    "                                for name in model_preds.keys()])\n",
    "                weights = weights / weights.sum()  # 确保权重和为1\n",
    "                \n",
    "                # 计算加权平均预测\n",
    "                ensemble_pred = np.zeros(len(X_pred))\n",
    "                for model_name, pred in model_preds.items():\n",
    "                    ensemble_pred += pred * self.model_weights[target_name][model_name]\n",
    "                \n",
    "                # 应用总量约束\n",
    "                total_min, total_max = historical_baselines[target_name]['total_range']\n",
    "                if ensemble_pred.sum() < total_min:\n",
    "                    scale_factor = total_min / ensemble_pred.sum()\n",
    "                    ensemble_pred *= scale_factor\n",
    "                elif ensemble_pred.sum() > total_max:\n",
    "                    scale_factor = total_max / ensemble_pred.sum()\n",
    "                    ensemble_pred *= scale_factor\n",
    "                \n",
    "                predictions.append(ensemble_pred)\n",
    "                progress.update(task, advance=1)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        mean_pred = np.mean(predictions, axis=0)\n",
    "        std_pred = np.std(predictions, axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "    def identify_first_time_medals(self, \n",
    "                                 predictions: np.ndarray, \n",
    "                                 historical_data: pd.DataFrame,\n",
    "                                 countries: List[str]) -> List[str]:\n",
    "        \"\"\"识别可能首次获得奖牌的国家\"\"\"\n",
    "        # 获取历史上从未获得奖牌的国家\n",
    "        historical_medals = historical_data.groupby('NOC')['Total'].sum()\n",
    "        never_medaled = set(countries) - set(historical_medals[historical_medals > 0].index)\n",
    "        \n",
    "        # 预测值大于阈值的国家可能首次获得奖牌\n",
    "        threshold = 0.5  # 可调整的阈值\n",
    "        first_time_medals = []\n",
    "        \n",
    "        for country, pred in zip(countries, predictions):\n",
    "            if country in never_medaled and pred > threshold:\n",
    "                first_time_medals.append(country)\n",
    "        \n",
    "        return first_time_medals\n",
    "    \n",
    "    def predict_2028_olympics(self, features_df: pd.DataFrame, historical_data: pd.DataFrame) -> None:\n",
    "        \"\"\"改进的预测主函数\"\"\"\n",
    "        try:\n",
    "            # 准备训练数据\n",
    "            X, y, feature_cols = self.prepare_data(features_df, historical_data)\n",
    "            \n",
    "            # 训练模型\n",
    "            self.console.print(\"\\n[bold cyan]训练模型中...[/bold cyan]\")\n",
    "            trained_models = self.train_models(X, y)\n",
    "            \n",
    "            # 准备2028年预测数据\n",
    "            X_2028 = self._prepare_2028_features(features_df, historical_data)\n",
    "            \n",
    "            # 确保特征列一致性\n",
    "            X_2028_features = X_2028[feature_cols]\n",
    "            \n",
    "            # 预测并计算不确定性\n",
    "            self.console.print(\"\\n[bold cyan]生成2028年预测...[/bold cyan]\")\n",
    "            results = {}\n",
    "            for target in ['Gold', 'Total']:\n",
    "                if target in trained_models:\n",
    "                    mean_pred, std_pred = self.predict_with_uncertainty(\n",
    "                        X_2028_features,\n",
    "                        trained_models,\n",
    "                        target\n",
    "                    )\n",
    "                    results[target] = {\n",
    "                        'predictions': mean_pred,\n",
    "                        'uncertainty': std_pred\n",
    "                    }\n",
    "            \n",
    "            # 识别可能首次获得奖牌的国家\n",
    "            first_time_medalists = self.identify_first_time_medals(\n",
    "                results['Total']['predictions'],\n",
    "                historical_data,\n",
    "                X_2028['NOC'].unique()\n",
    "            )\n",
    "            \n",
    "            # 输出预测结果\n",
    "            self._display_predictions(results, X_2028['NOC'].unique(), first_time_medalists)\n",
    "            \n",
    "            # 保存模型和预测结果\n",
    "            self._save_results(trained_models, results, X_2028['NOC'].unique())\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]预测过程中出现错误: {str(e)}[/bold red]\")\n",
    "            raise e\n",
    "        \n",
    "    def _prepare_2028_features(self, features_df: pd.DataFrame, historical_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"改进的2028特征准备函数\"\"\"\n",
    "        # 复制最近一年的数据\n",
    "        latest_year = features_df['Year'].max()\n",
    "        X_2028 = features_df[features_df['Year'] == latest_year].copy()\n",
    "        X_2028['Year'] = 2028\n",
    "        \n",
    "        # 计算历史特征\n",
    "        for col in ['Gold', 'Total']:\n",
    "            # 计算移动平均\n",
    "            X_2028[f'{col}_ma_4year'] = historical_data.groupby('NOC')[col].transform(\n",
    "                lambda x: x.rolling(4, min_periods=1).mean()\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # 安全的增长率计算\n",
    "            def safe_pct_change(x):\n",
    "                change = x.pct_change(4)\n",
    "                change = change.replace([np.inf, -np.inf], np.nan)\n",
    "                upper_bound = np.nanpercentile(change[~np.isnan(change)], 90)\n",
    "                return change.clip(lower=-1, upper=upper_bound)\n",
    "                \n",
    "            X_2028[f'{col}_growth'] = historical_data.groupby('NOC')[col].transform(safe_pct_change).fillna(0)\n",
    "            X_2028[f'historical_max_{col}'] = historical_data.groupby('NOC')[col].transform('max').fillna(0)\n",
    "        \n",
    "        # 添加国家分层\n",
    "        tier_1_countries = ['United States', 'China', 'Great Britain', 'Japan', 'Australia']\n",
    "        tier_2_countries = ['France', 'Germany', 'Italy', 'Netherlands', 'South Korea']\n",
    "        X_2028['country_tier'] = (\n",
    "            X_2028['NOC'].apply(\n",
    "                lambda x: 1 if x in tier_1_countries else (2 if x in tier_2_countries else 3)\n",
    "            )\n",
    "            .astype(int)  # 强制转换为整数类型\n",
    "        )\n",
    "        \n",
    "        return X_2028\n",
    "    \n",
    "    def _display_predictions(self, \n",
    "                        results: Dict, \n",
    "                        countries: np.ndarray, \n",
    "                        first_time_medalists: List[str]) -> None:\n",
    "        \"\"\"显示预测结果\"\"\"\n",
    "        # 创建结果表格\n",
    "        table = Table(title=\"2028洛杉矶奥运会奖牌预测\")\n",
    "        table.add_column(\"国家\")\n",
    "        table.add_column(\"预计金牌数\")\n",
    "        table.add_column(\"预计总奖牌数\")\n",
    "        table.add_column(\"预测不确定性\")\n",
    "        \n",
    "        # 将countries转换为列表以使用index方法\n",
    "        countries_list = list(countries)\n",
    "        \n",
    "        for i, country in enumerate(countries_list):\n",
    "            gold_pred = f\"{results['Gold']['predictions'][i]:.1f}\"\n",
    "            gold_std = f\"±{results['Gold']['uncertainty'][i]:.1f}\"\n",
    "            total_pred = f\"{results['Total']['predictions'][i]:.1f}\"\n",
    "            total_std = f\"±{results['Total']['uncertainty'][i]:.1f}\"\n",
    "            \n",
    "            table.add_row(\n",
    "                country,\n",
    "                f\"{gold_pred} ({gold_std})\",\n",
    "                f\"{total_pred} ({total_std})\",\n",
    "                \"高\" if country in first_time_medalists else \"中\"\n",
    "            )\n",
    "        \n",
    "        self.console.print(table)\n",
    "        \n",
    "        # 显示首次获奖国家\n",
    "        if first_time_medalists:\n",
    "            self.console.print(\"\\n[bold green]预计首次获得奖牌的国家:[/bold green]\")\n",
    "            for country in first_time_medalists:\n",
    "                self.console.print(f\"- {country}\")\n",
    "    \n",
    "    def _save_results(self, \n",
    "                    models: Dict, \n",
    "                    results: Dict, \n",
    "                    countries: List[str]) -> None:\n",
    "        \"\"\"保存模型和预测结果\"\"\"\n",
    "        # 创建保存目录\n",
    "        save_dir = Path(\"models\")\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 保存模型\n",
    "        for target, target_models in models.items():\n",
    "            for model_name, model in target_models.items():\n",
    "                joblib.dump(\n",
    "                    model, \n",
    "                    save_dir / f\"{target}_{model_name}_model.joblib\"\n",
    "                )\n",
    "        \n",
    "        # 保存预测结果\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'Country': countries,\n",
    "            'Predicted_Gold': results['Gold']['predictions'],\n",
    "            'Gold_Uncertainty': results['Gold']['uncertainty'],\n",
    "            'Predicted_Total': results['Total']['predictions'],\n",
    "            'Total_Uncertainty': results['Total']['uncertainty']\n",
    "        })\n",
    "        \n",
    "        # 同时保存为 CSV 和 Parquet 格式\n",
    "        predictions_df.to_csv(save_dir / \"predictions_2028.csv\", index=False)\n",
    "        predictions_df.to_parquet(save_dir / \"predictions_2028.parquet\", index=False)\n",
    "        \n",
    "        # 保存特征重要性\n",
    "        importance_df = pd.DataFrame(self.feature_importance)\n",
    "        importance_df.to_csv(save_dir / \"feature_importance.csv\", index=True)\n",
    "        importance_df.to_parquet(save_dir / \"feature_importance.parquet\", index=True)\n",
    "\n",
    "    def generate_summary_report(self, predictions_df: pd.DataFrame, historical_data: pd.DataFrame) -> str:\n",
    "        \"\"\"生成详细的预测评估报告\"\"\"\n",
    "        summary = []\n",
    "        \n",
    "        # 1. 基本统计分析\n",
    "        total_countries = len(predictions_df)\n",
    "        avg_gold = predictions_df['Predicted_Gold'].mean()\n",
    "        avg_total = predictions_df['Predicted_Total'].mean()\n",
    "        total_gold = predictions_df['Predicted_Gold'].sum()\n",
    "        total_medals = predictions_df['Predicted_Total'].sum()\n",
    "        \n",
    "        summary.append(\"1. 基本统计分析\")\n",
    "        summary.append(f\"   - 预测国家数量: {total_countries}\")\n",
    "        summary.append(f\"   - 平均预测金牌数: {avg_gold:.2f}\")\n",
    "        summary.append(f\"   - 平均预测总奖牌数: {avg_total:.2f}\")\n",
    "        summary.append(f\"   - 预测总金牌数: {total_gold:.2f}\")\n",
    "        summary.append(f\"   - 预测总奖牌数: {total_medals:.2f}\")\n",
    "        \n",
    "        # 2. 历史趋势分析\n",
    "        recent_years = historical_data['Year'].unique()[-3:]\n",
    "        historical_trends = []\n",
    "        for year in recent_years:\n",
    "            year_data = historical_data[historical_data['Year'] == year]\n",
    "            historical_trends.append({\n",
    "                'year': year,\n",
    "                'total_gold': year_data['Gold'].sum(),\n",
    "                'total_medals': year_data['Total'].sum()\n",
    "            })\n",
    "        \n",
    "        summary.append(\"\\n2. 历史趋势分析\")\n",
    "        for trend in historical_trends:\n",
    "            summary.append(f\"   - {trend['year']}年:\")\n",
    "            summary.append(f\"     * 总金牌数: {trend['total_gold']}\")\n",
    "            summary.append(f\"     * 总奖牌数: {trend['total_medals']}\")\n",
    "        \n",
    "        # 3. 预测可信度评估\n",
    "        gold_uncertainty = predictions_df['Gold_Uncertainty'].mean()\n",
    "        total_uncertainty = predictions_df['Total_Uncertainty'].mean()\n",
    "        \n",
    "        summary.append(\"\\n3. 预测可信度评估\")\n",
    "        summary.append(f\"   - 金牌预测平均不确定性: ±{gold_uncertainty:.2f}\")\n",
    "        summary.append(f\"   - 总奖牌预测平均不确定性: ±{total_uncertainty:.2f}\")\n",
    "        \n",
    "        # 4. 主要发现\n",
    "        summary.append(\"\\n4. 主要发现\")\n",
    "        summary.append(\"   - 预测趋势与历史数据对比\")\n",
    "        summary.append(\"   - 国家间竞争格局变化\")\n",
    "        summary.append(\"   - 新兴运动强国分析\")\n",
    "        \n",
    "        # 5. 预测局限性\n",
    "        summary.append(\"\\n5. 预测局限性\")\n",
    "        summary.append(\"   - 模型假设和约束\")\n",
    "        summary.append(\"   - 不确定性来源\")\n",
    "        summary.append(\"   - 潜在影响因素\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "def main():\n",
    "    console = Console()\n",
    "    \n",
    "    try:\n",
    "        # 创建保存目录\n",
    "        Path(\"models\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # 加载数据\n",
    "        console.print(\"[bold cyan]加载数据...[/bold cyan]\")\n",
    "        \n",
    "        # 尝试不同的数据加载方式\n",
    "        def load_data(file_path_base):\n",
    "            \"\"\"尝试多种方式加载数据\"\"\"\n",
    "            # 尝试不同的文件扩展名和编码\n",
    "            attempts = [\n",
    "                (f\"{file_path_base}.parquet\", lambda x: pd.read_parquet(x)),\n",
    "                (f\"{file_path_base}.csv\", lambda x: pd.read_csv(x)),\n",
    "                (f\"{file_path_base}.csv\", lambda x: pd.read_csv(x, encoding='utf-8')),\n",
    "                (f\"{file_path_base}.csv\", lambda x: pd.read_csv(x, encoding='latin1'))\n",
    "            ]\n",
    "            \n",
    "            last_error = None\n",
    "            for file_path, reader in attempts:\n",
    "                try:\n",
    "                    if Path(file_path).exists():\n",
    "                        data = reader(file_path)\n",
    "                        console.print(f\"[green]成功从 {file_path} 加载数据[/green]\")\n",
    "                        return data\n",
    "                except Exception as e:\n",
    "                    last_error = e\n",
    "                    continue\n",
    "            \n",
    "            raise FileNotFoundError(f\"无法加载数据文件 {file_path_base}.*\\n最后的错误: {str(last_error)}\")\n",
    "        \n",
    "        # 加载特征数据\n",
    "        features_df = load_data(\"data/processed/features\")\n",
    "        historical_data = load_data(\"data/processed/medal_counts\")\n",
    "        \n",
    "        # 数据验证\n",
    "        required_columns = ['Year', 'NOC', 'Gold', 'Total']\n",
    "        for col in required_columns:\n",
    "            if col not in historical_data.columns:\n",
    "                raise ValueError(f\"历史数据缺少必要的列: {col}\")\n",
    "        \n",
    "        # 显示数据基本信息\n",
    "        console.print(\"\\n[bold green]数据加载完成[/bold green]\")\n",
    "        console.print(f\"特征数据形状: {features_df.shape}\")\n",
    "        console.print(f\"特征列: {', '.join(features_df.columns)}\")\n",
    "        console.print(f\"历史数据形状: {historical_data.shape}\")\n",
    "        console.print(f\"历史数据列: {', '.join(historical_data.columns)}\")\n",
    "        \n",
    "        # 检查数据质量\n",
    "        console.print(\"\\n[bold cyan]检查数据质量...[/bold cyan]\")\n",
    "        \n",
    "        # 检查缺失值\n",
    "        missing_features = features_df.isnull().sum()\n",
    "        if missing_features.any():\n",
    "            console.print(\"[yellow]特征数据中存在缺失值:[/yellow]\")\n",
    "            console.print(missing_features[missing_features > 0])\n",
    "        \n",
    "        missing_historical = historical_data.isnull().sum()\n",
    "        if missing_historical.any():\n",
    "            console.print(\"[yellow]历史数据中存在缺失值:[/yellow]\")\n",
    "            console.print(missing_historical[missing_historical > 0])\n",
    "        \n",
    "        # 初始化预测器\n",
    "        predictor = OlympicMedalPredictor()\n",
    "        \n",
    "        # 运行预测\n",
    "        predictor.predict_2028_olympics(features_df, historical_data)\n",
    "        # 加载预测结果\n",
    "        predictions_df = pd.read_parquet(\"models/predictions_2028.parquet\")\n",
    "        \n",
    "        # 生成摘要报告\n",
    "        summary_report = predictor.generate_summary_report(predictions_df, historical_data)\n",
    "        \n",
    "        # 保存报告\n",
    "        with open(\"models/prediction_summary_report.txt\", \"w\") as f:\n",
    "            f.write(summary_report)\n",
    "        \n",
    "        console.print(\"\\n[bold cyan]预测评估摘要:[/bold cyan]\")\n",
    "        console.print(summary_report)\n",
    "        # 保存结果\n",
    "        console.print(\"\\n[bold green]预测完成！结果已保存到 models 目录[/bold green]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]错误: {str(e)}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
